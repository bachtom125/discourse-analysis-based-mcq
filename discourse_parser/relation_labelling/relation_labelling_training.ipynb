{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get and Preprocess Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29339, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>relation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>converted_relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GUM_academic_art</td>\n",
       "      <td>Aesthetic Appreciation and Spanish Art :</td>\n",
       "      <td>Insights from Eye - Tracking</td>\n",
       "      <td>elaboration-additional</td>\n",
       "      <td>13</td>\n",
       "      <td>Elaboration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           doc_name                                    nucleus  \\\n",
       "0  GUM_academic_art  Aesthetic Appreciation and Spanish Art :    \n",
       "\n",
       "                       satellite                relation  word_count  \\\n",
       "0  Insights from Eye - Tracking   elaboration-additional          13   \n",
       "\n",
       "  converted_relation  \n",
       "0        Elaboration  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/GUM/rst/total_relations_with_doc_name.csv')\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_switch = df.copy() # swith positions of nucleus and satellite\n",
    "# df_switch.rename(columns={'nucleus': 'satellite', 'satellite': 'nucleus'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = pd.concat([df, df_switch], ignore_index=True)\n",
    "combined_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_doc(dataset, test_range, doc_name_label='doc_name'):\n",
    "    \"\"\"Split dataset into train and test set, ensuring each doc stay reside in only one.\n",
    "\n",
    "    Args:\n",
    "        dataset (DataFrame):\n",
    "        test_range (tuple): (start, end) e.g. (0, 0.2) means the first 20%\n",
    "        doc_name_label (str): label of documents' name in the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    groups = dataset.groupby(doc_name_label)\n",
    "    documents = [group for _, group in groups]\n",
    "\n",
    "    test_start_idx = int(len(documents) * test_range[0])\n",
    "    test_end_idx = int(len(documents) * test_range[1])\n",
    "\n",
    "    test_docs = documents[test_start_idx:test_end_idx]\n",
    "    train_docs = documents[:test_start_idx] + documents[test_end_idx:]\n",
    "    \n",
    "    train_df = pd.concat(train_docs).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_docs).reset_index(drop=True)\n",
    "    assert len(set(train_df[doc_name_label])) + len(set(test_df[doc_name_label])) == len(set(dataset[doc_name_label]))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text = ['Attribution', 'Background', 'Cause', 'Condition', 'Contrast',\n",
    "       'Elaboration', 'Enablement', 'Evaluation', 'Explanation', 'Joint',\n",
    "       'Manner-Means', 'Same-Unit', 'Summary', 'Temporal',\n",
    "       'Textual-Organization', 'Topic-Change', 'Topic-Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_data(dataset, tokenizer, label_col='converted_relation'):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for row in dataset.iterrows():\n",
    "        tokens = tokenizer(row[1]['nucleus'], row[1]['satellite'], padding='max_length', truncation=True, return_tensors='pt')\n",
    " \n",
    "        input_ids.append(tokens['input_ids'][0])\n",
    "        token_type_ids.append(tokens['token_type_ids'][0])\n",
    "        attention_mask.append(tokens['attention_mask'][0])\n",
    "\n",
    "        \n",
    "    le = LabelEncoder()\n",
    "    le.fit(label_text)\n",
    "    labels = le.transform(dataset[label_col])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def get_dataset(dataset, tokenizer, label_col='converted_relation'):\n",
    "#     \"\"\"Turn dataframe into list(dict), dict with keys \"text\" and \"label\" \"\"\"\n",
    "\n",
    "#     # get text    \n",
    "#     separation_token = tokenizer.sep_token\n",
    "#     input_sentences = dataset.apply(lambda x: ''.join([x['nucleus'], separation_token, x['satellite']]), axis=1)\n",
    "#     np.array(input_sentences)\n",
    "\n",
    "#     # get labels\n",
    "#     le = LabelEncoder()\n",
    "#     le.fit(label_text)\n",
    "#     labels = le.transform(dataset[label_col])\n",
    "\n",
    "#     data = []   \n",
    "#     for text, label in zip(input_sentences, labels):\n",
    "#         datapoint = {'text': text, 'label': label}\n",
    "#         data.append(datapoint)\n",
    "#     data = np.array(data)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test set while preserving class distribution\n",
    "# access split data like this: for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "\n",
    "# WORKING: may try incorporating this with doc split \n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "# stk = StratifiedKFold(n_splits=5)\n",
    "# data_split = stk.split(input_sentences, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def freeze_encoder_layers(model, num_frozen_layers):\n",
    "#   \"\"\"Freezes the first `num_frozen_layers` of DeBERTa model.\n",
    "\n",
    "#   Args:\n",
    "#       model: The DeBERTa model to be fine-tuned.\n",
    "#       num_frozen_layers: The number of layers to freeze.\n",
    "#   \"\"\"\n",
    "#   for name, param in model.named_parameters():\n",
    "#     if name.startswith(\"deberta.encoder.layer.\") and int(name.split(\".\")[3]) < num_frozen_layers:\n",
    "#       param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def freeze_embeddings_layers(model):\n",
    "#   \"\"\"Freezes all embeddings-related layers of DeBERTa model. (no option for number of layers 'cause there's only one)\n",
    "\n",
    "#   Args:\n",
    "#       model: The DeBERTa model to be fine-tuned.\n",
    "#   \"\"\"\n",
    "#   for name, param in model.named_parameters():\n",
    "#     if name.startswith(\"deberta.embeddings.\"):\n",
    "#       param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_encoder_layers(model, 12)\n",
    "# freeze_embeddings_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(param.requires_grad, '-', name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred): \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# def tokenize_function(dataset):\n",
    "    # return tokenizer(dataset[\"text\"], padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23166' max='23950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23166/23950 6:03:35 < 12:18, 1.06 it/s, Epoch 4.84/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.693500</td>\n",
       "      <td>2.499886</td>\n",
       "      <td>0.054031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.428900</td>\n",
       "      <td>2.266045</td>\n",
       "      <td>0.145723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.216800</td>\n",
       "      <td>2.071317</td>\n",
       "      <td>0.201409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.056000</td>\n",
       "      <td>1.849533</td>\n",
       "      <td>0.273534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.831000</td>\n",
       "      <td>1.653843</td>\n",
       "      <td>0.388036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.682700</td>\n",
       "      <td>1.513986</td>\n",
       "      <td>0.438738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.578000</td>\n",
       "      <td>1.413495</td>\n",
       "      <td>0.476492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.442700</td>\n",
       "      <td>1.334139</td>\n",
       "      <td>0.507569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.382200</td>\n",
       "      <td>1.264921</td>\n",
       "      <td>0.526616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.218900</td>\n",
       "      <td>1.187122</td>\n",
       "      <td>0.575510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.156200</td>\n",
       "      <td>1.194515</td>\n",
       "      <td>0.598143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.085600</td>\n",
       "      <td>1.136053</td>\n",
       "      <td>0.609941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.103800</td>\n",
       "      <td>1.092261</td>\n",
       "      <td>0.642870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.053100</td>\n",
       "      <td>1.079852</td>\n",
       "      <td>0.645139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.008500</td>\n",
       "      <td>1.050132</td>\n",
       "      <td>0.654672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.029800</td>\n",
       "      <td>1.047341</td>\n",
       "      <td>0.652523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.038096</td>\n",
       "      <td>0.655644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>1.023215</td>\n",
       "      <td>0.664398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.984900</td>\n",
       "      <td>1.020235</td>\n",
       "      <td>0.663944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>1.019133</td>\n",
       "      <td>0.667968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>1.031328</td>\n",
       "      <td>0.669468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>1.000108</td>\n",
       "      <td>0.678533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>1.091539</td>\n",
       "      <td>0.666123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.825200</td>\n",
       "      <td>1.020896</td>\n",
       "      <td>0.673920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.812400</td>\n",
       "      <td>1.006268</td>\n",
       "      <td>0.677966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.826600</td>\n",
       "      <td>1.013233</td>\n",
       "      <td>0.681150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>1.034573</td>\n",
       "      <td>0.676328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.820600</td>\n",
       "      <td>1.031224</td>\n",
       "      <td>0.678398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.763600</td>\n",
       "      <td>1.040540</td>\n",
       "      <td>0.675046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>1.030241</td>\n",
       "      <td>0.685564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>1.028769</td>\n",
       "      <td>0.680331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>1.050046</td>\n",
       "      <td>0.684056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>1.035405</td>\n",
       "      <td>0.685659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>1.053486</td>\n",
       "      <td>0.685072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.753200</td>\n",
       "      <td>1.052320</td>\n",
       "      <td>0.687005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.029883</td>\n",
       "      <td>0.688970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.729300</td>\n",
       "      <td>1.031610</td>\n",
       "      <td>0.687432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.673000</td>\n",
       "      <td>1.035721</td>\n",
       "      <td>0.693838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>1.061265</td>\n",
       "      <td>0.689590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>1.068367</td>\n",
       "      <td>0.688808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.643100</td>\n",
       "      <td>1.062827</td>\n",
       "      <td>0.691005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.060809</td>\n",
       "      <td>0.691608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>1.067095</td>\n",
       "      <td>0.689859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.649900</td>\n",
       "      <td>1.064343</td>\n",
       "      <td>0.690054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>1.070566</td>\n",
       "      <td>0.689315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>1.073970</td>\n",
       "      <td>0.690028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# trainer object to perfrom training-related tasks\u001b[39;00m\n\u001b[1;32m     37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     39\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m eval_dataset\u001b[38;5;241m=\u001b[39mvalset,\n\u001b[1;32m     43\u001b[0m compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/e/TOM/HUST/20232/rst-relations-labeller/.venv/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", \n",
    "                                                                num_labels=17)\n",
    "model.to(torch.device('cuda'))\n",
    "\n",
    "# test_ranges = [(0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1)]\n",
    "\n",
    "train_df, test_df = split_dataset_by_doc(combined_df, (0.5, 0.7))\n",
    "\n",
    "tokenized_trainset = preprocess_data(train_df, tokenizer)\n",
    "tokenized_testset = preprocess_data(test_df, tokenizer)\n",
    "\n",
    "trainset = datasets.Dataset.from_dict(tokenized_trainset)\n",
    "valset = datasets.Dataset.from_dict(tokenized_testset)\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "output_dir=f\"./output_dataset_correct_order_correct_format\", \n",
    "learning_rate=4e-6,\n",
    "num_train_epochs=5,\n",
    "per_device_train_batch_size=5,\n",
    "per_device_eval_batch_size=5,\n",
    "warmup_steps=500,\n",
    "weight_decay=0.01,\n",
    "load_best_model_at_end=True,\n",
    "evaluation_strategy=\"steps\",\n",
    "metric_for_best_model=\"eval_loss\",\n",
    "greater_is_better=False,\n",
    "save_total_limit=3,\n",
    "save_steps=500,\n",
    "eval_steps=500)\n",
    "\n",
    "# trainer object to perfrom training-related tasks\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "tokenizer=tokenizer,\n",
    "args=training_args,\n",
    "train_dataset=trainset,\n",
    "eval_dataset=valset,\n",
    "compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=valset)\n",
    "print(\"Score: \", eval_results, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: To work out the evolutionary history, development and relationships among groups of organisms,[SEP]biologists compare the characteristics of living species in a process called phylogenetic analysis\n",
      "Label: Enablement\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "sample_n = \"\"\"To work out the evolutionary history, development and relationships among groups of organisms,\"\"\"\n",
    "sample_s = \"\"\"biologists compare the characteristics of living species in a process called phylogenetic analysis\"\"\"\n",
    "sample = sample_n + separation_token + sample_s\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(sample, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    output = cur_model(**tokens)\n",
    "    \n",
    "logits = output.logits\n",
    "logits = torch.Tensor.cpu(logits)\n",
    "prediction = int(np.argmax(logits))\n",
    "label = le.classes_[prediction]\n",
    "print(\"Sentence:\", sample)\n",
    "print(\"Label:\", label)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Helper Blocks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
