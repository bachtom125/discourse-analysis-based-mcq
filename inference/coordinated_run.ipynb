{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>distractor1</th>\n",
       "      <th>distractor2</th>\n",
       "      <th>distractor3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elaboration</td>\n",
       "      <td>It is a result of more severe economic problems or a downturn than the recession itself,</td>\n",
       "      <td>which is a slowdown in economic activity over the course of the normal business cycle of growing economy.</td>\n",
       "      <td>What is an economic depression?</td>\n",
       "      <td>Is a slowdown in economic activity over the course of the normal business cycle of growing economy.</td>\n",
       "      <td>Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.</td>\n",
       "      <td>Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.</td>\n",
       "      <td>Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elaboration</td>\n",
       "      <td>that may be named economic depression are part of economic cycles</td>\n",
       "      <td>where the slowdown of the economy follows the economic growth and vice versa.</td>\n",
       "      <td>What are the economic cycles?</td>\n",
       "      <td>The slowdown of the economy follows the economic growth and vice versa.</td>\n",
       "      <td>The economy slows down and grows faster.</td>\n",
       "      <td>The economy slows down and grows faster.</td>\n",
       "      <td>The economy slows down and grows faster.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relation  \\\n",
       "0  Elaboration   \n",
       "1  Elaboration   \n",
       "\n",
       "                                                                                    nucleus  \\\n",
       "0  It is a result of more severe economic problems or a downturn than the recession itself,   \n",
       "1                         that may be named economic depression are part of economic cycles   \n",
       "\n",
       "                                                                                                   satellite  \\\n",
       "0  which is a slowdown in economic activity over the course of the normal business cycle of growing economy.   \n",
       "1                              where the slowdown of the economy follows the economic growth and vice versa.   \n",
       "\n",
       "                          question  \\\n",
       "0  What is an economic depression?   \n",
       "1    What are the economic cycles?   \n",
       "\n",
       "                                                                                                answer  \\\n",
       "0  Is a slowdown in economic activity over the course of the normal business cycle of growing economy.   \n",
       "1                              The slowdown of the economy follows the economic growth and vice versa.   \n",
       "\n",
       "                                                                                                                                     distractor1  \\\n",
       "0  Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.   \n",
       "1                                                                                                       The economy slows down and grows faster.   \n",
       "\n",
       "                                                                                                                                     distractor2  \\\n",
       "0  Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.   \n",
       "1                                                                                                       The economy slows down and grows faster.   \n",
       "\n",
       "                                                                                                                                     distractor3  \n",
       "0  Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.  \n",
       "1                                                                                                       The economy slows down and grows faster.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ques_and_dis.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_tree_dict(pickle_path):\n",
    "    try: \n",
    "        with open(pickle_path, 'rb') as file:\n",
    "            tree = pickle.load(file)\n",
    "            return tree\n",
    "    except:\n",
    "        print(\"Can't open file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_id(tree_dict):\n",
    "    for key, item in tree_dict.items():\n",
    "        if item['pnode_id'] == -1:\n",
    "            root_id = key\n",
    "    return root_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelib\n",
    "\n",
    "def visualize_rst_tree(tree_dict, root_id, edu_list, new_relation=False, get_edu_text=False):\n",
    "    rst_tree = treelib.Tree()\n",
    "    relation_key = 'relation' if not new_relation else 'new_relation'\n",
    "    node_list = [root_id]\n",
    "\n",
    "    while node_list:\n",
    "        id = node_list.pop()\n",
    "        node = tree_dict[id]\n",
    "        if (tree_dict.get(node['lnode_id']) is None) and (tree_dict.get(node['rnode_id']) is None):\n",
    "            node_text = \" EDU \" + str(node['edu_span'])\n",
    "            if get_edu_text:\n",
    "                node_text += \": \" + edu_list[node['edu_span'][0] - 1]\n",
    "            rst_tree.create_node(node_text, id, parent=node['pnode_id'])\n",
    "        else:\n",
    "            node_text = node['node_form']\n",
    "\n",
    "            if node['node_form'] == 'NN':\n",
    "                node_text += \"-\" + tree_dict[node['rnode_id']][relation_key]\n",
    "            elif node['node_form'] == 'NS':\n",
    "                node_text += \"-\" + tree_dict[node['rnode_id']][relation_key]\n",
    "            elif node['node_form'] == 'SN':\n",
    "                node_text += \"-\" + tree_dict[node['lnode_id']][relation_key]\n",
    "            else:\n",
    "                raise ValueError(\"Unrecognized N-S form\")\n",
    "            \n",
    "            if rst_tree.get_node(node['pnode_id']) is not None:\n",
    "                rst_tree.create_node(node_text, id, parent=node['pnode_id'])\n",
    "            else:\n",
    "                rst_tree.create_node(node_text, id)\n",
    "                print(\"\\nNo parent at node: \", node_text, '\\n')\n",
    "\n",
    "        if tree_dict.get(node['rnode_id']) is not None:\n",
    "            node_list.append(node['rnode_id'])\n",
    "        if tree_dict.get(node['lnode_id']) is not None:\n",
    "            node_list.append(node['lnode_id'])\n",
    "\n",
    "    return rst_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edus_from_file(edu_path):\n",
    "    \"\"\"Get EDUs from .edu file and return a list of EDUs\n",
    "    \"\"\"\n",
    "    edus = []\n",
    "    try: \n",
    "        with open(edu_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                edus.append(line.rstrip('\\n'))\n",
    "        return edus    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(edu_list, tree_dict, root_id): # use this when unpickling\n",
    "    \"\"\"Extract text segments from (1 or several EDUs) for relation labeller to read from (and make predictions)\n",
    "\n",
    "    Args:\n",
    "        edu_list: list containing EDUs (list)\n",
    "        tree_dict: dict containing tree (dict)\n",
    "\n",
    "    Return:\n",
    "        Dict containing text of nucleus, satellite, and original relation from StageDP (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    segments = {'pnode_id': [], 'nucleus': [], 'satellite': [], 'original_relation': []} # if multi-nuclear, satellite represent second nucleus\n",
    "    node_list = [root_id]\n",
    "    while node_list:\n",
    "        id = node_list.pop()\n",
    "        node = tree_dict[id]\n",
    "\n",
    "        if (tree_dict.get(node['lnode_id']) is None) and (tree_dict.get(node['rnode_id']) is None): # node is EDU\n",
    "            continue\n",
    "    \n",
    "        left_edu_span = tree_dict[node['lnode_id']]['edu_span'] # tuple: (from, to)\n",
    "        right_edu_span = tree_dict[node['rnode_id']]['edu_span'] # tuple: (from, to)\n",
    "        \n",
    "        # get corresponding text segments\n",
    "        left_segment = \"\"\n",
    "        for edu in range(left_edu_span[0], left_edu_span[1] + 1):\n",
    "            left_segment += edu_list[edu - 1].strip() + ' '\n",
    "\n",
    "        right_segment = \"\"\n",
    "        for edu in range(right_edu_span[0], right_edu_span[1] + 1):\n",
    "            right_segment += edu_list[edu - 1].strip() + ' '\n",
    "\n",
    "        if node['node_form'] == 'NN':\n",
    "            nucleus = left_segment\n",
    "            satellite = right_segment\n",
    "            relation = tree_dict[node['rnode_id']]['relation']\n",
    "        elif node['node_form'] == 'NS':\n",
    "            nucleus = left_segment\n",
    "            satellite = right_segment\n",
    "            relation = tree_dict[node['rnode_id']]['relation']\n",
    "        elif node['node_form'] == 'SN':\n",
    "            nucleus = right_segment\n",
    "            satellite = left_segment\n",
    "            relation = tree_dict[node['lnode_id']]['relation']\n",
    "\n",
    "        segments['nucleus'].append(nucleus)\n",
    "        segments['satellite'].append(satellite)\n",
    "        segments['original_relation'].append(relation)\n",
    "        segments['pnode_id'].append(id)  \n",
    "        \n",
    "        if tree_dict.get(node['lnode_id']) is not None:\n",
    "            node_list.append(node['lnode_id'])\n",
    "        if tree_dict.get(node['rnode_id']) is not None:\n",
    "            node_list.append(node['rnode_id'])\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def add_new_relations_to_tree_dict(tree_dict, new_relations):\n",
    "    \"\"\"Extract text segments from (1 or several EDUs) for relation labeller to read from (and make predictions)\n",
    "\n",
    "    Args:\n",
    "        tree_dict: dict containing tree (dict)\n",
    "        new_relations: df containing parent id and new relations (and other components no considered in this method)\n",
    "\n",
    "    Return:\n",
    "        New modified tree_dict according to new relations identified\n",
    "    \"\"\"\n",
    "    tree_dict_c = copy.deepcopy(tree_dict)\n",
    "    for _, r in new_relations.iterrows():\n",
    "        p_id = r['pnode_id']\n",
    "        rel = r['new_relation']\n",
    "        if tree_dict_c[p_id]['node_form'] == 'NN':\n",
    "            tree_dict_c[tree_dict_c[p_id]['rnode_id']]['new_relation'] = rel\n",
    "            tree_dict_c[tree_dict_c[p_id]['lnode_id']]['new_relation'] = rel\n",
    "        elif tree_dict_c[p_id]['node_form'] == 'NS':\n",
    "            tree_dict_c[tree_dict_c[p_id]['rnode_id']]['new_relation'] = rel\n",
    "        elif tree_dict_c[p_id]['node_form'] == 'SN':\n",
    "            tree_dict_c[tree_dict_c[p_id]['lnode_id']]['new_relation'] = rel\n",
    "        \n",
    "    return tree_dict_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_text_file(text_path, text):\n",
    "    \"\"\"Write string in text to text_path. The text is to be analyzed using RST and generated questions from. \n",
    "\n",
    "    Args:\n",
    "        text_path (str): path of file to write to\n",
    "        text (str): text to write to (informational text to extract questions from)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(text_path, 'w') as f:\n",
    "            f.write(text)\n",
    "    except:\n",
    "        print(\"Can't open text file!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to start processing text\n",
    "# context_path = 'wikipedia_articles/economic_depression.txt'\n",
    "context_path = '../data/sample/article'\n",
    "with open(context_path, 'r') as f:\n",
    "    raw_original_text = f.read()\n",
    "\n",
    "# remove erronous characters\n",
    "raw_original_text = raw_original_text.replace(u\"\\u2018\", \"'\").replace(u\"\\u2019\", \"'\").replace(u\"\\u2013\", \"-\").replace(u\"\\u2014\", \"-\").replace(u\"\\u201C\", \"-\").replace(u\"\\u201D\", \"-\") \n",
    "text_path = \"../data/sample/article\"\n",
    "\n",
    "# write_to_text_file(text_path, raw_original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No parent at node:  NS-Elaboration \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to process new data, adjust paths if necessary\n",
    "\n",
    "pickle_path = \"../data/sample/article.pickle\"\n",
    "edu_path = \"../data/sample/article.edus\"\n",
    "\n",
    "tree_dict = get_tree_dict(pickle_path)\n",
    "root_id = get_root_id(tree_dict)\n",
    "edus = get_edus_from_file(edu_path)\n",
    "rst_tree = visualize_rst_tree(tree_dict, root_id, edus, get_edu_text=True)\n",
    "\n",
    "# print(rst_tree.show(stdout=False, sorting=False)) # uncomment to visualize RST tree\n",
    "# print(edus)\n",
    "\n",
    "segments = extract_segments(edus, tree_dict, root_id)\n",
    "df = pd.DataFrame(segments)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembly the whole piece of text from EDUs, to ensure allignment\n",
    "\n",
    "original_text = \"\"\n",
    "for edu in edus:\n",
    "    original_text += edu.strip() + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map \"Comparison\" to \"Join\" since GUM does not contain \"Comparison\"\n",
    "for row in df[df['original_relation'] == 'Comparison'].iterrows():\n",
    "    df.at[row[0], 'original_relation'] = \"Joint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Labeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/TOM/HUST/20232/question-generation/inference/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"../models/output_dataset_correct_order_fold_1/checkpoint-23000\"\n",
    "if 'tokenizer' not in locals(): # prevent accidental re-run of cell\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if 'model' not in locals(): # prevent accidental re-run of cell\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_text = ['Attribution', 'Background', 'Cause', 'Condition', 'Contrast',\n",
    "       'Elaboration', 'Enablement', 'Evaluation', 'Explanation', 'Joint',\n",
    "       'Manner-Means', 'Same-Unit', 'Summary', 'Temporal',\n",
    "       'Textual-Organization', 'Topic-Change', 'Topic-Comment']\n",
    "\n",
    "label_shorthand = ['Attr', 'Bckg', 'Cause', 'Cond', 'Contst',\n",
    "       'Elab', 'Enab', 'Eval', 'Expl', 'Joint',\n",
    "       'Man-Mean', 'Same-Un', 'Sum', 'Temp',\n",
    "       'Text-Org', 'Top-Chang', 'Top-Com']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(label_text)\n",
    "labels = le.transform(df.original_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding <sep> token between nucleus and satellite\n",
    "separation_token = \"[SEP]\"\n",
    "input_sentences = df.apply(lambda x: ''.join([x['nucleus'], separation_token, x['satellite']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge input sentence and labels onto one list (to form dataset object later)\n",
    "data = []\n",
    "for text in input_sentences:\n",
    "    datapoint = {'text': text}\n",
    "    data.append(datapoint)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "import datasets\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 32\n",
    "dataset = datasets.Dataset.from_list(list(data))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        tokens = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        output = model(**tokens)\n",
    "        logits = torch.Tensor.cpu(output.logits)\n",
    "        pred_labels.extend(np.argmax(logits, axis=-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = le.inverse_transform(pred_labels)\n",
    "df['new_relation'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 changed relations out of 10 (0.1)\n"
     ]
    }
   ],
   "source": [
    "# calculate porportion of changed labels\n",
    "\n",
    "diff = df.apply(lambda x: x['original_relation'] != x['new_relation'], axis=1)\n",
    "print(diff.sum(), \"changed relations out of\", df.shape[0], '(' + str(round(float(diff.sum()/df.shape[0]), 2)) + ')')\n",
    "# df[diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No parent at node:  NS-Elaboration \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fix old tree with new relations\n",
    "\n",
    "new_tree_dict = add_new_relations_to_tree_dict(tree_dict, df)\n",
    "new_rst_tree = visualize_rst_tree(new_tree_dict, get_root_id(new_tree_dict), edus, new_relation=True, get_edu_text=True)\n",
    "\n",
    "# print(new_rst_tree.show(stdout=False, sorting=False)) # for visualizing rst_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_cnt = pd.Series(pred_labels).value_counts()\n",
    "indices = pd.Series(label_shorthand).reindex(label_cnt.index, fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.yticks(np.arange(1, max(label_cnt) + 1))\n",
    "plt.bar(indices, label_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pnode_id</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>original_relation</th>\n",
       "      <th>new_relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pnode_id, nucleus, satellite, original_relation, new_relation]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find occurences of a specific relation\n",
    "\n",
    "df[df[\"new_relation\"] == \"Enablement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enablement'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "sep = tokenizer.sep_token\n",
    "text_n = \"She used the tool in the garden,\"\n",
    "text_s = \"so as to win.\"\n",
    "text = text_n + sep + text_s\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    output = model(**tokens)\n",
    "    logits = torch.Tensor.cpu(output.logits)\n",
    "    single_pred = int(np.argmax(logits, axis=-1))\n",
    "    \n",
    "le.inverse_transform([single_pred])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Incomplete Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INC_SOURCE_LENGTH = 600\n",
    "MAX_INC_TARGET_LENGTH = 128\n",
    "INC_PREFIX = \"make question:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "inc_model_path = \"../models/t5_base_incomplete_questions_wkdiswfla_max_len_600/not_filter_long_contexts/checkpoint-66000\"\n",
    "\n",
    "if 'inc_tokenizer' not in locals(): # prevent accidental re-run of cell\n",
    "    inc_tokenizer = T5Tokenizer.from_pretrained(inc_model_path)\n",
    "if 'inc_model' not in locals(): # prevent accidental re-run of cell\n",
    "    inc_model = T5ForConditionalGeneration.from_pretrained(inc_model_path)\n",
    "    inc_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_question(context, key, answer):\n",
    "    \"\"\"Complete question if needed, needs globally available model and tokenizer (\"inc_model\" and \"inc_tokenizer\")\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        question (str): \n",
    "        answer (str): \n",
    "    Return:\n",
    "        str: new question\n",
    "    \"\"\"\n",
    "   \n",
    "    inc_model.eval()\n",
    "\n",
    "    inputs = inc_tokenizer(text=f\"{INC_PREFIX} answer: {answer}, key: {key}, context: {context}\",\n",
    "                            max_length=MAX_INC_SOURCE_LENGTH,\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            return_tensors='pt').to('cuda')\n",
    "            \n",
    "    output_sequences = inc_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=MAX_INC_TARGET_LENGTH\n",
    "    )\n",
    "    \n",
    "    output = inc_tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "key = \"\"\"Why\"\"\" \n",
    "answer = \"\"\"The savings & loans and the leveraged buyout crises led to a severe depression in mid - to - late 1989.\"\"\"\n",
    "\n",
    "# new_question = complete_question(original_text, key, answer)\n",
    "# print(f\"Keyword: {key}\")\n",
    "# print(f\"Question: {new_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# test_set = pd.read_csv(\"Datasets/incomplete_questions/final_test_set.csv\")\n",
    "\n",
    "# test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # following the training script: get long questions only\n",
    "# test_condition = [len(q.split(\" \")) >= 10 for q in test_set['question']]\n",
    "# test_set = test_set[test_condition]\n",
    "\n",
    "# test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_testset(dataset):\n",
    "    prompts = [f\"{INC_PREFIX} answer: {answer}, context: {context}, key: {key}\" for context, key, answer in zip(dataset['context'], dataset['incomplete_question'], dataset['answer'])]\n",
    "    desired_output = list(dataset['question'])\n",
    "\n",
    "    inputs = inc_tokenizer(\n",
    "        text=prompts,\n",
    "        max_length=MAX_INC_SOURCE_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to('cuda')\n",
    "\n",
    "    return inputs # list['input_ids, attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_testset_with_no_key(dataset):\n",
    "    prompts = [f\"{INC_PREFIX} answer: {answer}, context: {context}, key: \" for context, answer in zip(dataset['context'], dataset['answer'])]\n",
    "    desired_output = list(dataset['question'])\n",
    "\n",
    "    inputs = inc_tokenizer(\n",
    "        text=prompts,\n",
    "        max_length=MAX_INC_SOURCE_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to('cuda')\n",
    "\n",
    "    return inputs # list['input_ids, attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "\n",
    "def compute_bleu(tokenized_dataset, labels, batch_size):\n",
    "    \"\"\"Compute BLEU metric with a tokenized test set. Need to have globally available inc_model and inc_tokenizer\n",
    "\n",
    "    Args:\n",
    "        tokenized_dataset (List): containing \"input_ids\" and \"attention_mask\" fields\n",
    "        labels (List[str]): containing reference questions\n",
    "    \"\"\"\n",
    "    inc_model.eval()\n",
    "    \n",
    "    data_len = len(tokenized_dataset['input_ids'])\n",
    "    bleus = []\n",
    "\n",
    "    for i in range(0, data_len, batch_size):\n",
    "        output_seqs = inc_model.generate(\n",
    "            input_ids=tokenized_dataset[\"input_ids\"][i:i+batch_size],\n",
    "            attention_mask=tokenized_dataset[\"attention_mask\"][i:i+batch_size],\n",
    "            max_length=MAX_INC_TARGET_LENGTH\n",
    "        )\n",
    "\n",
    "        outputs = inc_tokenizer.batch_decode(output_seqs, skip_special_tokens=True)\n",
    "\n",
    "        bleu = bleu_score.corpus_bleu(list_of_references=[[label] for label in labels[i:i+batch_size]], hypotheses=outputs, weights=(0, 0, 0, 1))\n",
    "        bleus.append(bleu)\n",
    "        \n",
    "    return np.array(bleus).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_test_set = preprocess_testset(test_set)\n",
    "# labels = test_set['question']\n",
    "\n",
    "# bleu_score = compute_bleu(tokenized_test_set, labels, batch_size=5)\n",
    "# bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bleu score with no key\n",
    "# tokenized_test_set = preprocess_testset_with_no_key(test_set)\n",
    "# labels = test_set['question']\n",
    "\n",
    "# bleu_score = compute_bleu(tokenized_test_set, labels, batch_size=5)\n",
    "# bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATIVE_PRONOUNS = ['who', 'that', 'whose', 'which']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj(clause, accept_pron=True, accept_expl=False):\n",
    "    \"\"\"Return subject of clause, None of none found\n",
    "\n",
    "    Args:\n",
    "        clause (str): \n",
    "        accept_expl (bool, optional): If take expletive as subject. Defaults to False.\n",
    "        accept_expl (bool, optional): If take pronoun as subject. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The subject\n",
    "    \"\"\"\n",
    "    doc = nlp(clause)\n",
    "\n",
    "    for token in doc:\n",
    "        if 'nsubj' in token.dep_:\n",
    "            if token.pos_ == \"PRON\" and not accept_pron:\n",
    "                continue\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if chunk.start <= token.i and token.i < chunk.end:\n",
    "                    return chunk.text\n",
    "        if accept_expl:\n",
    "            if 'expl' in token.dep_:\n",
    "                return token.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_verb(clause):\n",
    "    \"\"\"Get main verb of a clause, return None if none found.\n",
    "\n",
    "    Args:\n",
    "        clause (str): \n",
    "    Returns:\n",
    "        str:\n",
    "    \"\"\"\n",
    "    doc = nlp(clause)\n",
    "    root_tok = list(doc.sents)[-1].root\n",
    "\n",
    "    return root_tok.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_noun_chunk(clause):\n",
    "    \"\"\"Get last noun chunk of the clause, return None if none found.\n",
    "\n",
    "    Args:\n",
    "        clause (str): \n",
    "    Returns:\n",
    "        str:\n",
    "    \"\"\"\n",
    "    doc = nlp(clause)\n",
    "    if len(list(doc.noun_chunks)) != 0:\n",
    "        last_noun_chunk = list(doc.noun_chunks)[-1]\n",
    "\n",
    "        return last_noun_chunk.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING: make a more fool proof method of checking relative clause: inputing both source sents and text, and use pos tags to determine, refer to : is_dependent_clause()\n",
    "def check_relative_clause(text):\n",
    "    \"\"\"Return if the text (has to contain only one clause) is a relative clause, \n",
    "    relative clauses can start with \"which\", \"Ving, \"Ved\" (not including adverbial clause)\n",
    "\n",
    "    Args:\n",
    "        text\n",
    "    Return\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if (\"nsubj\" in token.dep_):\n",
    "                if (token.text.lower() in RELATIVE_PRONOUNS):\n",
    "                    return 1 # relative clause starting with relative pronouns\n",
    "                break\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if (token.pos_ == \"VERB\") and (token.tag_ in ['VBG', 'VBN']):\n",
    "                # WORKING: Change way to check tense, use tag_ instead of morph\n",
    "                if len(token.morph.get('Tense')) == 0:\n",
    "                    return 0 # not relative clause\n",
    "                if token.dep_ == \"ROOT\" and ((token.text.strip().endswith('ing')) or (token.tag_ == 'VBN')):\n",
    "                    return 2 # shortened relative clause (starting with Ving or Ved)\n",
    "            else: \n",
    "                return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundary(sent_doc, text_doc):\n",
    "    \"\"\"Find boundary indices of text_doc in sent_doc (given that text_doc is in sent_doc)\n",
    "\n",
    "    Args:\n",
    "        sent_doc (nlp Doc): \n",
    "        text_doc (nlp Doc): \n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    while start_ind < len(sent_doc):\n",
    "        if text_doc.text not in sent_doc[start_ind:].text:\n",
    "            break\n",
    "        start_ind += 1\n",
    "    start_ind -= 1\n",
    "\n",
    "    end_ind = len(sent_doc)\n",
    "    while end_ind > 0:\n",
    "        if (text_doc.text not in sent_doc[:end_ind].text) or (end_ind <= start_ind):\n",
    "            break\n",
    "        end_ind -= 1\n",
    "    end_ind += 1\n",
    "\n",
    "    return start_ind, end_ind\n",
    "\n",
    "def is_dependent_clause(src_sent, text):\n",
    "    # WORKING: may use this way to check relative clause as well\n",
    "    sent_doc = nlp(src_sent)\n",
    "    text_doc = nlp(text)   \n",
    "\n",
    "    start_ind, end_ind = find_boundary(sent_doc, text_doc)\n",
    "    if start_ind < 0 or end_ind > len(sent_doc):\n",
    "        print(\"\\nText not found in source sentence!\\n\")\n",
    "        return None\n",
    "\n",
    "    for token in sent_doc[start_ind:end_ind]:\n",
    "        if token.dep_ in ['acl', 'advcl', 'xcomp']: # omitted \"ccomp\", put back again if needed\n",
    "            if token.head.i in range(start_ind, end_ind):\n",
    "                return False\n",
    "            return True\n",
    "    return False     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def unshorten_relative_clause(original_sent, clause):\n",
    "    \"\"\"Unshorten relative clause (make sure it's a relative clause before calling this method) ending with Ving or Ved, convert them to which/who + V\n",
    "\n",
    "    Args:\n",
    "        clause (str): relative clause containing Ving or Ved\n",
    "        original_sent (str): original sentence containing that clause\n",
    "    Return:\n",
    "        tuple(str, str): modified clause and source sentence\n",
    "    \"\"\"\n",
    "\n",
    "    clause_doc = nlp(clause)\n",
    "    text_doc = nlp(original_sent)\n",
    "    vb = \"\"\n",
    "\n",
    "    for token in clause_doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if (token.pos_ == \"VERB\") and (token.tag_ in ['VBG', 'VBN']): # finds Ving or Ved\n",
    "                vb = token\n",
    "\n",
    "    for v_token in text_doc:\n",
    "        # find verb in original sentence\n",
    "        if v_token.text == vb.text.strip():\n",
    "            c_i = vb.i\n",
    "            t_i = v_token.i\n",
    "            is_verb = True\n",
    "            while c_i < len(clause_doc) and t_i < len(text_doc):\n",
    "                if clause_doc[c_i].text.strip() != text_doc[t_i].text.strip():\n",
    "                    is_verb = False\n",
    "                    break\n",
    "                c_i += 1\n",
    "                t_i += 1\n",
    "            if is_verb:\n",
    "                break\n",
    "    \n",
    "    pointed_noun = v_token.head\n",
    "    pointed_noun_chunk = None\n",
    "    for chunk in text_doc.noun_chunks:\n",
    "        if chunk.start <= pointed_noun.i and pointed_noun.i < chunk.end:\n",
    "            pointed_noun_chunk = chunk\n",
    "    \n",
    "    if pointed_noun_chunk:\n",
    "        pointed_root_noun = pointed_noun_chunk.root\n",
    "    else:\n",
    "        pointed_root_noun = pointed_noun\n",
    "    \n",
    "    rel_pro = \"which\" \n",
    "    # not a fool-proof way to determine if noun is person\n",
    "    if pointed_root_noun.ent_type_:\n",
    "        if pointed_root_noun.ent_type_ == \"PERSON\":\n",
    "            rel_pro = \"who\"\n",
    "\n",
    "    # check plurality of noun/pronoun and conjugate accordingly\n",
    "    # only applicable for Present Tense, not for past or others\n",
    "    if pointed_root_noun.pos_.startswith(\"NOUN\"): # noun\n",
    "        plurality = pointed_root_noun.tag_ == \"NNS\"\n",
    "    elif \"PRON\" in pointed_root_noun.pos_:  # pronoun\n",
    "        plurality = (pointed_root_noun.lemma_ == \"we\") or (pointed_root_noun.lemma_ == \"you\") or (pointed_root_noun.lemma_ == \"they\") or ((pointed_root_noun.lemma_ == \"I\"))\n",
    "    else: # WORKING: for other cases where relative pronoun does not point to a noun, but a verb or a clause\n",
    "        plurality = False # temporary solution\n",
    "\n",
    "    if not plurality:\n",
    "        if vb.tag_ == 'VBG':\n",
    "            conj_vb = p.plural_noun(vb.lemma_) # get singular conjugation (plural_noun() method works with verbs too)\n",
    "        else:\n",
    "            if pointed_root_noun.text.strip() == \"I\":\n",
    "                aux = 'am'\n",
    "            else:\n",
    "                aux = 'is'\n",
    "            conj_vb = aux + ' ' + vb.text\n",
    "    else:\n",
    "        if vb.tag_ == 'VGB':\n",
    "            conj_vb = p.plural_verb(vb.lemma_) # get plural conjugation\n",
    "        else:\n",
    "            if pointed_root_noun.text.strip() == \"I\":\n",
    "                aux = 'am'\n",
    "            else:\n",
    "                aux = 'are'\n",
    "            conj_vb = aux + ' ' + vb.text\n",
    "\n",
    "    fixed_clause = clause.replace(vb.text, rel_pro + ' ' +  conj_vb, 1)# replace only the first occurence of the verb\n",
    "    fixed_sent = original_sent.replace(clause.strip(), fixed_clause.strip(), 1)\n",
    "    return  (fixed_sent, fixed_clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I saw Tom which walks at midnight', 'which walks at midnight')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unshorten_relative_clause(\"I saw Tom walking at midnight\", \"walking at midnight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING: Need to handle all types of shortened relative clauses, for now, only Ving and Ved is covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_one_clause(text, count_relative_clause=True):\n",
    "    \"\"\"Check if input text is one clause or multiple.\n",
    "        NOTE: If not multiple clause, the method returns true, so does not account for the case of not a full clause, just check whether multiple clauses or not, cause a EDU is usually at least a clause semantically. \n",
    "\n",
    "    Args:\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # check how many subjects\n",
    "    has_subj = False\n",
    "    for token in doc:\n",
    "        if \"nsubj\" in token.dep_:\n",
    "            if not count_relative_clause:\n",
    "                if token.text.lower() in ['who', 'whom', 'whose', 'which', 'that']:\n",
    "                    continue\n",
    "                if token.head.dep_ in ['relcl', 'acl', 'ccomp']:\n",
    "                    continue\n",
    "            if has_subj:\n",
    "                return False\n",
    "            else:\n",
    "                has_subj = True\n",
    "    return True # not return has_subj, so that even no subject will be 1 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_single_sentence(text):\n",
    "    \"\"\"Determine if input text is one single sentence (one main subject and one main verb)\n",
    "\n",
    "    Args:\n",
    "        text (str): \n",
    "\n",
    "    Returns:\n",
    "        Boolean: \n",
    "    \"\"\"\n",
    "    # Parse the input text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize flags for subject and main verb\n",
    "    subject_count = 0\n",
    "    verb_count = 0\n",
    "    \n",
    "    # Iterate through the tokens in the parsed document\n",
    "    for token in doc:\n",
    "        # Check for a subject\n",
    "        if \"nsubj\" in token.dep_:\n",
    "            subject_count += 1\n",
    "        # Check for the main verb\n",
    "        if token.pos_ == \"VERB\" and token.dep_ not in [\"pcomp\", \"relcl\", \"acl\", \"ccomp\"]:\n",
    "            verb_count += 1\n",
    "    \n",
    "    # Determine if the text is a single clause\n",
    "    return verb_count == 1 and subject_count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_aux(sentence):\n",
    "    \"\"\"Check if sentence has auxiliary verb.\n",
    "    Input one sentence only.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if \"AUX\" in token.pos_:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_shorthand = {\n",
    "    \"'s\": \"is\",\n",
    "    \"'re\": \"are\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"'d\": \"had\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"n't\": \"not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_aux_to_beginning(sentence):\n",
    "    \"\"\"Move auxiliary verb to the beginning of sentence (to form question).\n",
    "    Input one sentence only. Make sure it has aux verb. Make sure sentence starts with subject.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    aux = \"\"\n",
    "    for token in doc:\n",
    "        if \"AUX\" in token.pos_:\n",
    "            aux = token.text\n",
    "            break\n",
    "    assert len(aux)\n",
    "\n",
    "    if aux.strip() in auxiliary_shorthand:\n",
    "        new_aux = auxiliary_shorthand[aux]\n",
    "        new_sent = new_aux + ' ' + sentence.strip().replace(aux, '', 1).replace(sentence[0], sentence[0].lower(), 1)\n",
    "    else:\n",
    "        new_sent = aux + ' ' + sentence.strip().replace(aux, '', 1).replace(sentence[0], sentence[0].lower(), 1)\n",
    "\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can the person on the right  be her father, I can do it if not.\n"
     ]
    }
   ],
   "source": [
    "# move aux test run\n",
    "\n",
    "txt = \"The person on the right can be her father, I can do it if not.\"\n",
    "if has_aux(txt):\n",
    "    print(move_aux_to_beginning(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_verb(sentence):\n",
    "    \"\"\"Check if sentence has normal verb.\n",
    "    Input one sentence only.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if \"VERB\" in token.pos_:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_and_replace_aux_for_verb(sentence):\n",
    "    \"\"\"Put appropriate aux at beginnging of clause \n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    main_verb = None\n",
    "\n",
    "    for token in doc:\n",
    "        if \"VERB\" in token.pos_:\n",
    "            main_verb = token\n",
    "            break\n",
    "            \n",
    "    if not main_verb:\n",
    "        print(\"\\nCan't find main verb in\", sentence, '!\\n')\n",
    "        return \"\"\n",
    "    \n",
    "    # check plurality\n",
    "    pointed_noun = main_verb.head\n",
    "    if pointed_noun.pos_.startswith(\"NOUN\"): # noun\n",
    "        plurality = pointed_noun.tag_ == \"NNS\"\n",
    "    elif pointed_noun.pos_.startswith(\"PRP\"):  # pronoun\n",
    "        plurality = (pointed_noun.lemma_ == \"we\") or (pointed_noun.lemma_ == \"you\") or (pointed_noun.lemma_ == \"they\")\n",
    "    else: # WORKING: for other cases where relative pronoun does not point to a noun, but a verb or a clause\n",
    "        plurality = True # temporary solution\n",
    "\n",
    "    # check tense\n",
    "    tense = \"present\" if main_verb.tag_ in ['VBZ', 'VBP'] else 'past'\n",
    "    # get aux\n",
    "    aux = {\n",
    "      \"present\": \"do\" if plurality else \"does\",\n",
    "      \"past\": \"did\",\n",
    "    }.get(tense)\n",
    "\n",
    "    # replace appropriate auxilary\n",
    "    new_sent = sentence.replace(main_verb.text, main_verb.lemma_, 1)\n",
    "    new_sent = aux + ' ' + new_sent\n",
    "\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did he hit down.\n"
     ]
    }
   ],
   "source": [
    "# move verb test run\n",
    "\n",
    "txt = \"he hit down.\"\n",
    "if has_verb(txt):\n",
    "    print(choose_and_replace_aux_for_verb(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ending_special_chars(sentence):\n",
    "    \"\"\"Remove ending non-word characters of a sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to be stripped\n",
    "\n",
    "    Returns:\n",
    "        str: stripped sentence \n",
    "    \"\"\"\n",
    "    sen_len = len(sentence)\n",
    "    for i in range(sen_len - 1, -1, -1):\n",
    "        char = sentence[i]\n",
    "\n",
    "        # check if the character is a punctuation mark\n",
    "        if char.isalnum():\n",
    "            return sentence\n",
    "        else:\n",
    "            sentence = sentence[:i]\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_special_chars(sentence):\n",
    "    \"\"\"Remove leading non-word characters of a sentence\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to be stripped\n",
    "\n",
    "    Returns:\n",
    "        str: stripped sentence \n",
    "    \"\"\"\n",
    "\n",
    "    start_ind = 0\n",
    "    for i in range(0, len(sentence)):\n",
    "        char = sentence[i]\n",
    "        # check if the character is a punctuation mark\n",
    "        if char.isalnum():\n",
    "            break\n",
    "        else:\n",
    "            start_ind = i + 1\n",
    "\n",
    "    return sentence[start_ind: ].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentence\n",
    "\n",
    "    Args:\n",
    "        text (str): text to be splited\n",
    "    Return: \n",
    "        list[str]: split text\n",
    "    \"\"\"\n",
    "    sents = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\", text)\n",
    "    sents = [sent for sent in sents if len(sent.strip())]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paras(text, deli):\n",
    "    paras = text.split(deli)\n",
    "    paras = [para.strip() for para in paras if len(para.strip()) != 0]\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_para_id(paras, text):\n",
    "    \"\"\"Find paragraph index of text\n",
    "\n",
    "    Args:\n",
    "        paras (list[str]): list of paragraphs\n",
    "        text (str): text to find\n",
    "    Return:\n",
    "        int: para id, -1 if not found\n",
    "    \"\"\"\n",
    "\n",
    "    for p_i in range(len(paras)):\n",
    "        if paras[p_i].find(text.strip()) != -1:\n",
    "            return p_i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtext(text, key, length):\n",
    "    \"\"\"Get subtext of text with length specified surrounding key.\\\n",
    "          0.4 of length will be before the key, 0.6 will be after. \n",
    "\n",
    "    Args:\n",
    "        text (str): \n",
    "        length (str): \n",
    "    Returns:\n",
    "        str: the subtext\n",
    "    \"\"\"\n",
    "    bef_len = int(0.4 * length)\n",
    "    af_len = length - bef_len\n",
    "\n",
    "    text_words = text.split()\n",
    "    key_words = key.split()\n",
    "    \n",
    "    bef_words = text[:text.find(key)].split()\n",
    "    pos_bef = len(bef_words)\n",
    "    pos_af = pos_bef + len(key_words)\n",
    "\n",
    "    start_ind = 0\n",
    "    end_ind = len(text_words)\n",
    "\n",
    "    remainder_start = 0\n",
    "    remainder_end = 0\n",
    "\n",
    "    # get start index to extract\n",
    "    if pos_bef > bef_len:\n",
    "        start_ind = pos_bef - bef_len\n",
    "    else:\n",
    "        start_ind = 0\n",
    "        remainder_start = bef_len - pos_bef\n",
    "\n",
    "    # get end index to extract\n",
    "    if pos_af + af_len <= len(text_words):\n",
    "        end_ind = pos_af + af_len\n",
    "    else:\n",
    "        end_ind = len(text_words)\n",
    "        remainder_end = pos_af + af_len - len(text_words)\n",
    "\n",
    "    # if both ends are enough or have remainders\n",
    "    if (remainder_start != 0 and remainder_end != 0) or (remainder_start == 0 and remainder_end == 0):\n",
    "        return ' '.join(text_words[start_ind:end_ind])\n",
    "\n",
    "    # if only one of two ends have remainder\n",
    "    if remainder_start == 0:\n",
    "        start_ind = max(0, start_ind - remainder_end)\n",
    "    if remainder_end == 0:\n",
    "        end_ind = min(len(text_words), end_ind + remainder_start)\n",
    "\n",
    "    return ' '.join(text_words[start_ind:end_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_source_sents(sents, text, span=2):\n",
    "    \"\"\"Find the sentences of which the text is a part. \n",
    "\n",
    "    Args:\n",
    "        sents (str): \n",
    "        text (str): \n",
    "        span (int): span of sentences to the left to return\n",
    "    \"\"\"\n",
    "    start_ind = 0\n",
    "    while start_ind < len(sents):\n",
    "        if text not in ''.join(sents[start_ind:]):\n",
    "            break\n",
    "        start_ind += 1\n",
    "    start_ind -= 1\n",
    "\n",
    "    end_ind = len(sents) # exclusive\n",
    "    while end_ind > 0:\n",
    "        if (text not in ''.join(sents[:end_ind])) or (end_ind <= start_ind):\n",
    "            break\n",
    "        end_ind -= 1\n",
    "    end_ind += 1\n",
    "\n",
    "    if (start_ind < 0) or (end_ind > len(sents)):\n",
    "        print(f\"\\nCan't find source sentences of {text}. Returning the text\\n\")\n",
    "        return text\n",
    "    \n",
    "    src_sents = ''.join(sents[max(start_ind - span, 0):end_ind])\n",
    "    return src_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subject_to_relative_clause(original_sent, clause):\n",
    "    \"\"\"Find and Prepend (with modifications) the subject of relative clause that does not contain one\n",
    "\n",
    "    Args:\n",
    "        original_sent (str): sentence from which the clause is extracted\n",
    "        clause (str): clause for which to find subject\n",
    "    Return:\n",
    "        str: subject\n",
    "    \"\"\"\n",
    "    doc = nlp(original_sent)\n",
    "    subj = \"\"\n",
    "    clause_start_ind = original_sent.find(clause)\n",
    "    for token in doc:\n",
    "        if (len(subj)):\n",
    "            break\n",
    "        if (token.dep_ in ['relcl', 'acl']) and (token.idx >= clause_start_ind) and (token.idx < (clause_start_ind + len(clause))): # relative clause is noun modifier\n",
    "            for chunk in doc.noun_chunks:\n",
    "                if token.head.i >= chunk.start and token.head.i < chunk.end:\n",
    "                    subj = chunk.text\n",
    "                    break\n",
    "        \n",
    "        # WORKING: relative clause is verb/adverb/adjective modifier, not sure if it's necessary tho\n",
    "        # 'cause adverbial clauses are often in relations that do not require unshortening of clause\n",
    "        if (token.dep_ in ['advcl', 'ccomp']) and (token.idx >= clause_start_ind) and (token.idx < (clause_start_ind + len(clause))): \n",
    "            return None\n",
    "            # for chunk in doc.noun_chunks:\n",
    "            #     if token.head.i >= chunk.start and token.head.i < chunk.end:\n",
    "            #         subj = chunk.text\n",
    "            #         break\n",
    "                    \n",
    "    if not len(subj): # not found subject\n",
    "        print(\"\\nCan't find subject of\", original_sent, \"!\\n\")\n",
    "        return None         \n",
    "    \n",
    "    for token in doc:\n",
    "        if (\"nsubj\" in token.dep_) and (token.text.lower() in RELATIVE_PRONOUNS):\n",
    "            new_clause = clause.replace(token.text, subj) # contains more nuances (where -> in + N, which -> N, who -> N)\n",
    "            new_sent = original_sent.replace(clause.strip(), new_clause.strip(), 1)\n",
    "            return new_sent, new_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subject_to_fragmented_clause(original_sent, clause):\n",
    "    \"\"\"Find and Prepend (with modifications) the subject of a clause that does not contain one, the clause has to contain a verb.\n",
    "\n",
    "    Args:\n",
    "        original_sent (str): sentence from which the clause is extracted\n",
    "        clause (str): clause for which to find subject\n",
    "    Return:\n",
    "        str: subject\n",
    "    \"\"\"\n",
    "    doc = nlp(original_sent)\n",
    "    clause_doc = nlp(clause)\n",
    "           \n",
    "    # get main verb of clause\n",
    "    root_token = None\n",
    "    for token in clause_doc:\n",
    "        if \"nsubj\" in token.dep_:\n",
    "            return original_sent, clause\n",
    "\n",
    "        if token.dep_ == \"ROOT\" and \"VERB\" in token.pos_:\n",
    "            root_token = token\n",
    "            break\n",
    "    if root_token is None:\n",
    "        return original_sent, clause\n",
    "    \n",
    "    subj = None\n",
    "    main_verb = None\n",
    "    clause_start_ind = original_sent.find(clause)\n",
    "    for token in doc:\n",
    "        if (token.text == root_token.text) and (token.idx >= clause_start_ind) and (token.idx < (clause_start_ind + len(clause))):\n",
    "            main_verb = token \n",
    "            # token found here is verb of clause, now get verb of the missing part of the clause\n",
    "            other_verb = token.head\n",
    "            for c in other_verb.children:\n",
    "                if \"nsubj\" in c.dep_:\n",
    "                    subj = c\n",
    "                    break\n",
    "                    \n",
    "    if subj is None or main_verb is None:\n",
    "        print(f\"\\nCan't find subject for {clause}. Return the original clause.\\n\")\n",
    "        return original_sent, clause\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        if subj.i >= chunk.start and subj.i < chunk.end:\n",
    "            subj = chunk\n",
    "            break\n",
    "    \n",
    "    subj = subj.text\n",
    "    subj_char = list(subj)\n",
    "    subj_char[0] = subj_char[0].lower()\n",
    "    subj = ''.join(subj_char)\n",
    "    \n",
    "    # get starting point to replace, have main_verb, could be aux before main verb\n",
    "    for child in main_verb.children:\n",
    "        if 'aux' in child.dep_:\n",
    "            main_verb = child\n",
    "            break\n",
    "    \n",
    "    new_clause = clause.replace(main_verb.text, subj + ' ' + main_verb.text)\n",
    "    new_sent = original_sent.replace(clause.strip(), new_clause.strip(), 1)\n",
    "\n",
    "    return new_sent, new_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "text = \"The five factors continued to be supported both conceptually and statistically across major regions of the world,\"\n",
    "# sent = find_source_sents(sents, text, 1)\n",
    "# add_subject_to_fragmented_clause(sent, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_conjunction(original_sent, text):\n",
    "    \"\"\"Remove leading conjunctions from text, return intact if cannot find any\n",
    "\n",
    "    Args:\n",
    "        original_sent (_type_): _description_\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    conj = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PUNCT', 'SYM']:\n",
    "            continue\n",
    "        if \"CONJ\" not in token.pos_:\n",
    "            return (original_sent, text)\n",
    "        conj = token.text\n",
    "        break\n",
    "\n",
    "    new_text = text.replace(conj.strip(), '', 1)\n",
    "    new_sent = original_sent.replace(text, new_text)\n",
    "\n",
    "    return (new_sent, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_adverb(original_sent, text):\n",
    "    \"\"\"Remove leading adverb from text, return intact if cannot find any\n",
    "\n",
    "    Args:\n",
    "        original_sent (_type_): _description_\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    if text is None:\n",
    "        print(\"This is fucking it: \", original_sent)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PUNCT', 'SYM']:\n",
    "            continue\n",
    "        if token.pos_ != 'ADV':\n",
    "            return (original_sent, text)\n",
    "        adv = token.text\n",
    "        break\n",
    "\n",
    "    new_text = text.replace(adv.strip(), '', 1)\n",
    "    new_sent = original_sent.replace(text, new_text)\n",
    "\n",
    "    return (new_sent, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOURSE_MARKERS = [\n",
    "  'accordingly', 'additionally', 'afterward', 'also',\n",
    "  'although', 'as a final point', 'as a result', 'assuming that',\n",
    "  'besides', 'but also', 'compared to', 'consequently', 'conversely', 'despite',\n",
    "  'even though', 'finally', 'first', 'firstly', 'for example', 'for instance',\n",
    "  'for the purpose of', 'furthermore', 'hence', 'however', 'if', 'importantly',\n",
    "  'in addition', 'in case', 'in conclusion', 'in contrast', 'by contrast', 'in fact',\n",
    "  'in order to', 'in other words', 'in the event that', 'in the same way',\n",
    "  'indeed', 'just as', 'lastly', 'likewise', 'moreover', 'namely',\n",
    "  'nevertheless', 'next', 'nonetheless', 'not only', 'of course', 'on condition that',\n",
    "  'on the contrary', 'on the one hand', 'on the other hand', 'otherwise', 'plus', 'previously',\n",
    "  'provided that', 'second', 'secondly', 'similarly', 'similarly to', 'since',\n",
    "  'so', 'so long as', 'as long as', 'provided that', 'provided', 'so that', 'specifically', 'subsequently',\\\n",
    "  'such as', 'that is to say', 'that is'\n",
    "  'then', 'therefore', 'third', 'thirdly', 'thus', 'to conclude', 'to illustrate',\n",
    "  'to put it differently', 'to sum up', 'ultimately', 'undoubtedly', 'unless',\n",
    "  'while', 'with the aim of', 'yet', 'then', 'and then', 'and'\n",
    "  'as a consequence', 'as a result', 'in which', \"at which\", \"where\", \"followed by\", \"following\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_leading_discourse_marker(original_sent, text):\n",
    "    \"\"\"Remove leading discourse markers from text, return intact if cannot find any\n",
    "\n",
    "    Args:\n",
    "        original_sent (_type_): _description_\n",
    "        text (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    discourse_marker = None\n",
    "    min_pos = len(text)\n",
    "    for dm in DISCOURSE_MARKERS:\n",
    "        find_result = text.lower().find(dm)\n",
    "        if find_result > -1:\n",
    "            if find_result < min_pos:\n",
    "                min_pos = find_result\n",
    "                discourse_marker = dm\n",
    "            if find_result == min_pos and len(dm) > len(discourse_marker):\n",
    "                discourse_marker = dm\n",
    "    \n",
    "    # check if found marker stand at the beginning of text\n",
    "        \n",
    "    if discourse_marker is not None:\n",
    "        for i in range(0, min_pos):\n",
    "            if text[i].isalnum() or text[min_pos + len(discourse_marker)].isalnum():\n",
    "                discourse_marker = None\n",
    "                break\n",
    "            \n",
    "    if not discourse_marker:\n",
    "        return (original_sent, text)\n",
    "\n",
    "    pattern = re.compile(discourse_marker, re.IGNORECASE)\n",
    "\n",
    "    new_text = pattern.sub(\"\", text, 1)\n",
    "    new_sent = original_sent.replace(text, new_text)\n",
    "    \n",
    "    return (new_sent, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be integrated\n",
    "\n",
    "def replace_substr(text, substring, start_ind, end_ind):\n",
    "  \"\"\"Replace part of text with specified index [start_ind, end_ind) with substring\n",
    "  \"\"\"\n",
    "  try:\n",
    "    assert(len(substring)  == end_ind - start_ind)\n",
    "  except:\n",
    "    print('Text:', text, '--', sep='')\n",
    "    print('Substring:', substring, '--', sep='')\n",
    "\n",
    "  text_l = list(text)\n",
    "  text_l[start_ind:end_ind] = list(substring)\n",
    "\n",
    "  return ''.join(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_ref(index, clusters):\n",
    "    \"\"\"Check if current index is in one of the references\n",
    "    \n",
    "    Args:\n",
    "        index (int): index to check\n",
    "        clusters (list(list(tuple))): list of cluster, each cluster containing a list of tuple correponding to indices of the references\n",
    "    Return:\n",
    "        tuple (verdict, (start, end), (ref_token_start, ref_token_end)): -1 both index if not found, ref_token is token to relace\n",
    "    \"\"\"\n",
    "    for cluster in clusters:\n",
    "        for token in cluster:\n",
    "            if cluster.index(token) == 0:\n",
    "                continue\n",
    "            if index >= token[0] and index < token[1]:\n",
    "                ref_token = (cluster[0][0], cluster[0][1])\n",
    "                return True, token, ref_token\n",
    "            \n",
    "    return False, (-1, -1), (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relative_clause_type(clause):\n",
    "    \"\"\"Check which type of relative clause \"clause\" is: \n",
    "    - which + V + clause (present subject is sufficient for being clause)\n",
    "    - which + V + (not clause)\n",
    "\n",
    "    Args:\n",
    "        sent (str): \n",
    "        clause (str): \n",
    "    Return: 0 or 1, -1 if not relative clause expected (not start with which + V), then it could be an adverbial clause\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(clause.strip())\n",
    "\n",
    "    i = 0\n",
    "    while i < len(doc):\n",
    "        token = doc[i]\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" not in token.pos_: \n",
    "                return -1\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    i += 1 \n",
    "    while i < len(doc):\n",
    "        token = doc[i]\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"VERB\" not in token.pos_ and \"AUX\" not in token.pos_: \n",
    "                return -1        \n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "    \n",
    "    # if reach here, is expected relative clause type (which + V)\n",
    "    i += 1\n",
    "    t_i = i # use to this to check subject and not modify i\n",
    "    while t_i < len(doc):\n",
    "        token = doc[t_i]\n",
    "        if 'nsubj' in token.dep_:\n",
    "            return 0\n",
    "        t_i += 1\n",
    "\n",
    "    return 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m427.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./.venv/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in ./.venv/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/04/2024 00:42:33 - INFO - \t missing_keys: []\n",
      "07/04/2024 00:42:33 - INFO - \t unexpected_keys: []\n",
      "07/04/2024 00:42:33 - INFO - \t mismatched_keys: []\n",
      "07/04/2024 00:42:33 - INFO - \t error_msgs: []\n",
      "07/04/2024 00:42:33 - INFO - \t Model Parameters: 590.0M, Transformer: 434.6M, Coref head: 155.4M\n"
     ]
    }
   ],
   "source": [
    "# to be integrated\n",
    "from fastcoref import FCoref, LingMessCoref\n",
    "if 'coref_resolver' not in locals(): # prevent accidental re-run of cell\n",
    "    coref_resolver = LingMessCoref(device='cuda:0')\n",
    "\n",
    "def resolve_coreference(original_sent, text):\n",
    "    \"\"\"Perfrom coreference resolution and replace corresponding text.\n",
    "\n",
    "    Args:\n",
    "        original_sent (str): Sentence the text was derived froms\n",
    "        text (str): Target text\n",
    "    \"\"\"\n",
    "\n",
    "    text_ind = original_sent.find(text) # starting index of text in original text\n",
    "    coref_preds = coref_resolver.predict(texts=[original_sent])\n",
    "    coref_clusters = coref_preds[0].get_clusters(as_strings=False)\n",
    "    new_sent = []\n",
    "    new_text = []\n",
    "\n",
    "    # interate string left to right while appending current char to a new list\n",
    "    # if current index in one of the token in one of the clusters, add the replacement to the list, keep the text intact, to know what index are at\n",
    "    i = 0\n",
    "\n",
    "    # if referred word is a verb, use have to notice and discard the sentence.\n",
    "    while i < len(original_sent):\n",
    "        find_result = is_in_ref(i, coref_clusters)\n",
    "        if find_result[0]:\n",
    "            token = find_result[1]\n",
    "            token_ref = find_result[2]\n",
    "            new_sent.append(original_sent[token_ref[0]:token_ref[1]])\n",
    "            if (i >= text_ind and i < text_ind + len(text)):\n",
    "                if (text_ind <= token[0] and token[1] <= text_ind + len(text)):\n",
    "                    new_text.append(original_sent[token_ref[0]:token_ref[1]])\n",
    "                else:\n",
    "                    new_text.append(original_sent[i:text_ind + len(text)])\n",
    "            i = token[1]\n",
    "        else:\n",
    "            new_sent.append(original_sent[i])\n",
    "            if i >= text_ind and i < text_ind + len(text):\n",
    "                new_text.append(original_sent[i])\n",
    "            i += 1\n",
    "\n",
    "    return ''.join(new_sent), ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/04/2024 00:42:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00,  1.85 examples/s]\n",
      "07/04/2024 00:42:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:03<00:00,  3.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CorefResult(text=\"Transformers process input sequences in parallel, ...\", clusters=[['Transformers', 'them']])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coref test run    \n",
    "txt = \"Transformers process input sequences in parallel, making the them highly efficient for training and inference\"\n",
    "# sents = split_into_sentences(original_text)\n",
    "# src = find_source_sents(sents, txt, 5)\n",
    "# resolve_coreference(src, txt)\n",
    "coref_preds = coref_resolver.predict(texts=[txt])\n",
    "coref_clusters = coref_preds[0].get_clusters(as_strings=False)\n",
    "coref_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_relative_pronoun(original_sent, text):\n",
    "    \"\"\"Remove relative pronouns from text, make sure it's a relative clause first\n",
    "\n",
    "    Args:\n",
    "        original_sent (str): \n",
    "        text (str): \n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if (\"nsubj\" in token.dep_):\n",
    "                if (token.text.lower() in RELATIVE_PRONOUNS):\n",
    "                    rel_pro = token.text\n",
    "\n",
    "    if len(rel_pro) == 0:\n",
    "        return original_sent, text\n",
    "    \n",
    "    new_text = text.replace(rel_pro.strip(), '', 1).strip()\n",
    "    new_sent = original_sent.replace(text, new_text).strip()\n",
    "    \n",
    "    return new_sent, new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(original_text, text, retain_dm=False, retain_conj=False, retain_adv=False, add_subject_to_rel_clause=False, resolve_coref=False, unshorten_rel=False, remove_rel_pron=True, get_full_sentence=True):\n",
    "    \"\"\"Perform necessary transformation steps.\n",
    "\n",
    "    Args:\n",
    "        text (str): raw text\n",
    "    Return:\n",
    "        (str): full clause from text\n",
    "    \"\"\"\n",
    "    sents = split_into_sentences(original_text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # coreferen ce resolution\n",
    "    src_3_sents = find_source_sents(sents, text, 3)\n",
    "    if resolve_coref:\n",
    "        src_3_sents, text = resolve_coreference(src_3_sents, text)\n",
    "    src_sent = find_source_sents(split_into_sentences(src_3_sents), text, 0)\n",
    "\n",
    "    # if no source sentence found\n",
    "    if src_sent is None:\n",
    "        return \"\", text # system just use text, not original_sent anyway\n",
    "\n",
    "    # removal of irrelavent components\n",
    "    if not retain_dm:\n",
    "        src_sent, text = remove_leading_discourse_marker(src_sent, text)    \n",
    "    if not retain_conj:\n",
    "        src_sent, text = remove_leading_conjunction(src_sent, text)\n",
    "    if not retain_adv: \n",
    "        src_sent, text = remove_leading_adverb(src_sent, text)\n",
    "    src_sent = remove_leading_special_chars(src_sent)\n",
    "    text = remove_leading_special_chars(text)\n",
    "    # src_3_sents = remove_ending_special_chars(src_3_sents)\n",
    "    # text = remove_ending_special_chars(text)\n",
    "\n",
    "    # handle single relative clause\n",
    "    if is_one_clause(text, count_relative_clause=False):\n",
    "        cl_type = check_relative_clause(text)\n",
    "        if cl_type != 0: # is relative clause\n",
    "            if cl_type == 2:\n",
    "                if unshorten_rel:\n",
    "                    src_sent, text = unshorten_relative_clause(src_sent, text)\n",
    "            if add_subject_to_rel_clause:\n",
    "                # WORKING: temporary solution before fixing add_subject_to_relative_clause()\n",
    "                if add_subject_to_relative_clause(src_sent, text):\n",
    "                    src_sent, text = add_subject_to_relative_clause(src_sent, text)\n",
    "            if remove_rel_pron:\n",
    "                src_sent, text = remove_relative_pronoun(src_sent, text)\n",
    "        # is not relative clause:\n",
    "        elif not is_single_sentence(text):\n",
    "            if get_full_sentence:\n",
    "                src_sent, text = add_subject_to_fragmented_clause(src_sent, text)\n",
    "\n",
    "    return src_sent, text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_pipeline(original_text, \"and thus cannot be within the thinking thing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def postprocess_question(ques):\n",
    "    \"\"\"Clean up the question after being generated.\n",
    "    Pipeline: Clean double spaces, clean extra punctuations\n",
    "\n",
    "    Args:\n",
    "        ques (str): generated question\n",
    "    Returns:\n",
    "        str: new question\n",
    "    \"\"\"\n",
    "\n",
    "    puncts_to_remove = ['.', ',', '!']\n",
    "\n",
    "    ques_c = list(ques)\n",
    "    for i in range(len(ques_c) - 1, 0, -1):\n",
    "        if ques_c[i].isalnum():\n",
    "            break\n",
    "\n",
    "        if ques_c[i] in puncts_to_remove:\n",
    "            ques_c.pop(i)\n",
    "    \n",
    "    # not adding extra space before question mark, used to have to for the complete question model to run well, but now use only keyword, not incomplete questions anymore\n",
    "    # for i in range(len(ques_c) - 1, 0, -1):\n",
    "    #     if ques_c[i] == '?':\n",
    "    #         ques_c.insert(i, ' ')\n",
    "    #         break\n",
    "\n",
    "    new_ques = ''.join(ques_c)\n",
    "    new_ques = re.sub(r'\\s+', ' ', new_ques)\n",
    "    return new_ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_answer(ans):\n",
    "    \"\"\"Clean up the answer after being generated.\n",
    "    Pipeline: Clean double spaces, clean extra punctuations, capitalize first word.\n",
    "\n",
    "    Args:\n",
    "        ans (str): generated answer\n",
    "    Returns:\n",
    "        str: new answer\n",
    "    \"\"\"\n",
    "\n",
    "    ending_puncts = ['.', '!']\n",
    "    ans = ans.strip()\n",
    "    \n",
    "    ans_c = list(ans)\n",
    "    has_punct = False\n",
    "    for i in range(len(ans_c) - 1, 0, -1):\n",
    "        if ans_c[i].isalnum():\n",
    "            break\n",
    "\n",
    "        if ans_c[i] in ending_puncts:\n",
    "            if not has_punct:\n",
    "                has_punct = True\n",
    "                continue\n",
    "        ans_c.pop(i)\n",
    "        \n",
    "    if not has_punct:\n",
    "        ans_c.append('.')\n",
    "        \n",
    "    for i in range(len(ans_c)):\n",
    "        if len(ans_c[i].strip()) == 0:\n",
    "            continue\n",
    "        ans_c[i] = ans_c[i].upper()\n",
    "        ans_c = ans_c[i:]\n",
    "        break\n",
    "\n",
    "    new_ans = ''.join(ans_c)\n",
    "    new_ans = re.sub(r'\\s+', ' ', new_ans)\n",
    "    return new_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But sometimes humans just literally are not able to do it, to have an algorithm do this.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test run\n",
    "\n",
    "postprocess_answer(\"But sometimes humans just literally are not able to do it, to have an algorithm do this:,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cause_question_type_0(nucleus, satellite): # WORKING: nucleus cause satellite (but now model interpret both directions, needs fixing)\n",
    "    \"\"\"Make question based on CAUSE relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting:exactly the same as type_1\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def cause_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on CAUSE relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which caused the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"    \n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def cause_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on CAUSE relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(satellite):\n",
    "        new_sate = move_aux_to_beginning(satellite)\n",
    "    else: \n",
    "        new_sate = choose_and_replace_aux_for_verb(satellite)\n",
    "    question = \"Why \" + new_sate.strip() + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "                                             \n",
    "def generate_cause_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate question based on Cause relation. \\\n",
    "        Nucleus being the Cause, which is the answer.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "\n",
    "    Returns:\n",
    "        Tuple(Boolean, Tuple(str, str)): Boolean dictates if questions are plausible. \\\n",
    "            The second tuple is the question-answer pair. \n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, \n",
    "                                          add_subject_to_rel_clause=False, resolve_coref=False)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite, resolve_coref=False)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, resolve_coref=True)\n",
    "\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "        if is_one_clause(satellite, count_relative_clause=False):\n",
    "            cl_type = check_relative_clause(satellite)\n",
    "            if cl_type != 0: # is relative clause\n",
    "                rel_type = check_relative_clause_type(satellite)\n",
    "                if rel_type == 0:\n",
    "                    question, answer = cause_question_type_0(nucleus, satellite)\n",
    "                    return (True, (question, answer))\n",
    "                elif rel_type == 1:\n",
    "                    question, answer = cause_question_type_1(nucleus, satellite)\n",
    "                    return (True, (question, answer))\n",
    "        else:\n",
    "            question, answer =  (\"\", \"\")\n",
    "            return (False, (question, answer))\n",
    "        question, answer =  cause_question_type_2(nucleus, satellite)\n",
    "        return (True, (question, answer))\n",
    "   \n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        key = \"Why\"\n",
    "        last_sate_noun_chunk = get_last_noun_chunk(full_satellite_pair[1])\n",
    "        if last_sate_noun_chunk is not None:\n",
    "            key = key + ' ' + last_sate_noun_chunk\n",
    "       \n",
    "        answer = nucleus\n",
    "        question = complete_question(context, key, answer) \n",
    "        \n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrast relation: ask what's different between two subjects.\n",
    "# retain discourse markers\n",
    "    \n",
    "def make_contrast_question_with_two_clauses(nucleus, satellite):\n",
    "    n_subj = get_subj(nucleus)\n",
    "    s_subj = get_subj(satellite)\n",
    "   \n",
    "    if (n_subj is None) or (s_subj is None):\n",
    "        print(\"\\nCan't find subject!\\n\")\n",
    "        return (\"\", \"\")\n",
    "     \n",
    "    if n_subj.strip().lower() == s_subj.strip().lower():\n",
    "        print(\"\\nSame subjects for nucleus and satellite!\\n\")\n",
    "        return (\"\", \"\")\n",
    "    \n",
    "    question = \"What is the difference between \" + n_subj + \" and \" + s_subj + '?'\n",
    "    answer = nucleus.strip() + ' ' + satellite.strip()\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_contrast_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate question based on Contrast relation.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "\n",
    "    Returns:\n",
    "        Tuple(Boolean, Tuple(str, str)): Boolean dictates if questions are plausible. \\\n",
    "            The second tuple is the question-answer pair. \n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=False)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=False, resolve_coref=False)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "        if is_one_clause(nucleus, count_relative_clause=False) and is_one_clause(satellite, count_relative_clause=False):\n",
    "            ques, ans = make_contrast_question_with_two_clauses(nucleus, satellite)\n",
    "            return (True, (ques, ans))\n",
    "        return (False, (\"\", \"\")) # more than 1 clause\n",
    "   \n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        # check if subjects are different\n",
    "        n_subj = get_subj(nucleus)\n",
    "        s_subj = get_subj(satellite)\n",
    "\n",
    "        if n_subj and s_subj:\n",
    "            if (n_subj.strip().lower() == s_subj.strip().lower()):\n",
    "                return (False, (\"\", \"\"))\n",
    "        \n",
    "        # currently take full_answer (to feed to complete question model) to be the same as answer, \\\n",
    "        # which is basically the same as original nucleus and satellite, with no context added, \\\n",
    "        # might change later if needed.\n",
    "\n",
    "        # full_answer = full_nucleus_pair[1].strip() + ' ' + full_satellite_pair[1].strip()\n",
    "        if context.find(nucleus) < context.find(satellite):           \n",
    "            full_answer = nucleus.strip() + ' ' + satellite.strip()\n",
    "            answer = nucleus.strip() + ' ' + satellite.strip()\n",
    "        else:\n",
    "            full_answer = satellite.strip() + ' ' + nucleus.strip()\n",
    "            answer = satellite.strip() + ' ' + nucleus.strip()\n",
    "        question = complete_question(context, \"How different\", full_answer)\n",
    "\n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test run\n",
    "sate = \"Elves and dwarfs are commonly mentioned and appear to be connected,\"\n",
    "nuc = \"but their attributes are vague and the relation between the two is ambiguous.\"\n",
    "\n",
    "# generate_contrast_question(original_text, nuc, sate, use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition relation\n",
    "\n",
    "def condition_question_type_0(nucleus, satellite): \n",
    "    \"\"\"Make question based on condition relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting:exactly the same as type_1\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What condition\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def condition_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on condition relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which conditiond the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What condition\", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def condition_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on condition relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(satellite):\n",
    "        new_sate = move_aux_to_beginning(satellite)\n",
    "    else: \n",
    "        new_sate = choose_and_replace_aux_for_verb(satellite)\n",
    "    question = \"In what condition \" + new_sate.strip() + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "                                             \n",
    "def generate_condition_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate question based on Condition relation.\\\n",
    "    Nucleus is the answer.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "\n",
    "    Returns:\n",
    "        Tuple(Boolean, Tuple(str, str)): Boolean dictates if questions are plausible. \\\n",
    "            The second tuple is the question-answer pair. \n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=False)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite, resolve_coref=True)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    # extract nucleus and satellite\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "        if is_one_clause(satellite, count_relative_clause=False):\n",
    "            cl_type = check_relative_clause(satellite)\n",
    "            if cl_type != 0: # is relative clause\n",
    "                rel_type = check_relative_clause_type(satellite)\n",
    "                if rel_type == 0:\n",
    "                    ques, ans =  condition_question_type_0(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "                elif rel_type == 1:\n",
    "                    ques, ans =  condition_question_type_1(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "            else: # 1 clause, not relative clause \n",
    "                ques, ans =  condition_question_type_2(nucleus, satellite)\n",
    "                return (True, (ques, ans))\n",
    "        else:\n",
    "            return (False, (\"\", \"\"))\n",
    "   \n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        # currently take full_answer (to feed to complete question model) to be the same as answer, \\\n",
    "        # which is basically the same as original nucleus, with no context added, \\\n",
    "        # might change later if needed.\n",
    "\n",
    "        # full_answer = full_satellite_pair[1].strip()\n",
    "        full_answer = satellite.strip()\n",
    "        \n",
    "        question = complete_question(context, \"Under what condition\", full_answer)\n",
    "        answer = satellite.strip()\n",
    "\n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "sate = \"that would not have been remotely economically feasible \"\n",
    "nuc = \"if they had to be done by humans\"\n",
    "# generate_condition_question(original_text, nuc, sate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enablement relation\n",
    "# difference to manner-means: enablement encapsulate manner-means, as all means can \"enable\" the goal,\n",
    "# but enablement also contains situational aid, an event lead (may not intentionally) to another event\n",
    "    \n",
    "# enablement relation\n",
    "\n",
    "def enablement_question_type_0(nucleus, satellite): \n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting: exactly the same as type_1\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What \", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def enablement_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which enablementd the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(satellite)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = satellite.replace(rel_pro.strip(), \"What \", 1) + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def enablement_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(satellite):\n",
    "        new_sate = move_aux_to_beginning(satellite)\n",
    "    else: \n",
    "        new_sate = choose_and_replace_aux_for_verb(satellite)\n",
    "    question = \"How \" + new_sate.strip() + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def enablement_question_type_3(nucleus, satellite): \n",
    "    \"\"\"Make question based on enablement relationship\n",
    "    Type 2: satellite (result) is: relative clause but not start with relative pronoun (most probably adverbial clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    question = \"What can be done \" + satellite.strip() + '?'\n",
    "    answer = nucleus.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_enablement_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate question based on Enablement relation.\\\n",
    "    Nucleus (the enabler) forms the question. f\n",
    "    Satellite (the enablee) is the answer.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "\n",
    "    Returns:\n",
    "        Tuple(Boolean, Tuple(str, str)): Boolean dictates if questions are plausible. \\\n",
    "            The second tuple is the question-answer pair. \n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, add_subject_to_rel_clause=True, \n",
    "                                          resolve_coref=True)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=False, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=False, unshorten_rel=True, get_full_sentence=False)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    # extract nucleus and satellite\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "        if is_one_clause(nucleus, count_relative_clause=False):\n",
    "            cl_type = check_relative_clause(nucleus)\n",
    "            if cl_type != 0: # is relative clause\n",
    "                rel_type = check_relative_clause_type(nucleus)\n",
    "                if rel_type == 0:\n",
    "                    ques, ans = enablement_question_type_0(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "                elif rel_type == 1:\n",
    "                    ques, ans = enablement_question_type_1(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "            else:\n",
    "                if is_dependent_clause(nucleus_pair[0], nucleus):\n",
    "                    ques, ans = enablement_question_type_3(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "                ques, ans = enablement_question_type_2(nucleus, satellite)\n",
    "                return (True, (ques, ans))\n",
    "        return (False, (\"\", \"\"))\n",
    "   \n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        # currently take full_answer (to feed to complete question model) to be the same as answer, \\\n",
    "        # which is basically the same as original nucleus, with no context added, \\\n",
    "        # might change later if needed.\n",
    "\n",
    "        # full_answer = full_nucleus_pair[1].strip()\n",
    "        last_noun_chunk = get_last_noun_chunk(full_nucleus_pair[1])\n",
    "        main_verb = get_main_verb(full_nucleus_pair[1])\n",
    "        key = \"For what purpose\"\n",
    "\n",
    "        if last_noun_chunk is not None:\n",
    "            key = key + ' ' + last_noun_chunk\n",
    "        # if main_verb is not None:\n",
    "        #     key = key + ' ' + main_verb\n",
    "\n",
    "        full_answer = satellite.strip() \n",
    "        question = complete_question(context, key, full_answer)\n",
    "        answer = satellite.strip()\n",
    "\n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "nuc = \"\"\"She rides to battle\"\"\"\n",
    "sate = \"to choose among the slain and brings her chosen to her afterlife field Folkvangr.\"\n",
    "\n",
    "# generate_enablement_question(original_text, nuc, sate, use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manner-Means relation\n",
    "    \n",
    "# manner-means relation\n",
    "# difference to manner-means: manner-means encapsulate manner-means, as all means can \"enable\" the goal,\n",
    "# but manner-means also contains situational aid, an event lead (may not intentionally) to another event\n",
    "    \n",
    "# manner-means relation\n",
    "\n",
    "def means_question_type_0(nucleus, satellite): \n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 0: satellite (result) is: relative clause: which + verb + clause (e.g. which made him happy.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    # experimenting: exactly the same as type_1\n",
    "    doc = nlp(nucleus)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = nucleus.replace(rel_pro.strip(), \"What method \", 1) + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "\n",
    "def means_question_type_1(nucleus, satellite):\n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 1: satellite (result) is: relative clause: which + verb + (not clause) (e.g. which caused the noise.)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "    doc = nlp(nucleus)\n",
    "    rel_pro = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['SPACE', 'PUNCT']:\n",
    "            if \"PRON\" in token.pos_:\n",
    "                rel_pro = token.text\n",
    "                break\n",
    "        \n",
    "    assert len(rel_pro) > 0\n",
    "    question = nucleus.replace(rel_pro.strip(), \"What method \", 1) + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def means_question_type_2(nucleus, satellite): \n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 2: satellite (result) is: full clause (not relative clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    if has_aux(nucleus):\n",
    "        new_nuc = move_aux_to_beginning(nucleus)\n",
    "    else: \n",
    "        new_nuc = choose_and_replace_aux_for_verb(nucleus)\n",
    "    question = \"By what method \" + new_nuc.strip() + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def means_question_type_3(nucleus, satellite): \n",
    "    \"\"\"Make question based on manner-means relationship\n",
    "    Type 2: satellite (result) is: relative clause but not start with relative pronoun (most probably adverbial clause)\n",
    "\n",
    "    Args:\n",
    "        nucleus (str): nucleus stripped of ending punctuation and spaces \n",
    "        satellite (str): satellite stripped of ending punctuation and spaces \n",
    "    Returns:p \n",
    "        tuple(ques, ans)\n",
    "    \"\"\"\n",
    "\n",
    "    question = \"What strategy can be employed \" + nucleus.strip() + '?'\n",
    "    answer = satellite.strip() + '.'\n",
    "\n",
    "    return (question, answer)\n",
    "\n",
    "def generate_means_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate question based on Manner-Means relation.\\\n",
    "    Satellite (the means) is the answer. \n",
    "    Nucleus (the end) forms the question.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "\n",
    "    Returns:\n",
    "        Tuple(Boolean, Tuple(str, str)): Boolean dictates if questions are plausible. \\\n",
    "            The second tuple is the question-answer pair. \n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, add_subject_to_rel_clause=True, \n",
    "                                          resolve_coref=True)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=False, resolve_coref=False)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    # extract nucleus and satellite\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "        if is_one_clause(nucleus, count_relative_clause=False):\n",
    "            cl_type = check_relative_clause(nucleus)\n",
    "            if cl_type != 0: # is relative clause\n",
    "                rel_type = check_relative_clause_type(nucleus)\n",
    "                if rel_type == 0:\n",
    "                    ques, ans =  means_question_type_0(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "                elif rel_type == 1:\n",
    "                    ques, ans =  means_question_type_1(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "            else:\n",
    "                if is_dependent_clause(nucleus_pair[0], nucleus):\n",
    "                    ques, ans =  means_question_type_3(nucleus, satellite)\n",
    "                    return (True, (ques, ans))\n",
    "                ques, ans =  means_question_type_2(nucleus, satellite)\n",
    "                return (True, (ques, ans))\n",
    "        return (False, (\"\", \"\"))\n",
    "    \n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        # currently take full_answer (to feed to complete question model) to be the same as answer, \\\n",
    "        # which is basically the same as original nucleus, with no context added, \\\n",
    "        # might change later if needed.\n",
    "\n",
    "        # full_answer = full_nucleus_pair[1].strip()\n",
    "        full_answer = satellite.strip()\n",
    "\n",
    "        key = \"How\"\n",
    "        last_noun_chunk = get_last_noun_chunk(full_nucleus_pair[1])\n",
    "        if last_noun_chunk is not None:\n",
    "            key = key + ' ' + last_noun_chunk\n",
    "\n",
    "        question = complete_question(context, key, full_answer)\n",
    "        answer = satellite.strip()\n",
    "\n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "\n",
    "nuc = \"\"\"scholars have identified elements of Germanic mythology reaching as far back as Proto - Indo - European mythology.\"\"\"\n",
    "sate = \"By way of comparative mythology sand historical linguistics,\"\n",
    "sub_context = get_subtext(original_text, nuc, 500) # get 500 words\n",
    "\n",
    "# generate_means_question(sub_context, nuc, sate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate a question based on the Temporal relation.\n",
    "    Satellite (later event) is the answer.\n",
    "    Nucleus (preceding event) forms the questiom.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "        use_template (bool, optional): Defaults to False.\n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, add_subject_to_rel_clause=True, \n",
    "                                          resolve_coref=True)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite,\n",
    "                                            add_subject_to_rel_clause=False, resolve_coref=False)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    # extract nucleus and satellite\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "       # WORKING: make template for this relation\n",
    "        return (False, (\"\", \"\"))\n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        # currently take full_answer (to feed to complete question model) to be the same as answer, \\\n",
    "        # which is basically the same as original nucleus, with no context added, \\\n",
    "        # might change later if needed.\n",
    "\n",
    "        # full_answer = full_nucleus_pair[1].strip()\n",
    "        full_answer = satellite.strip()\n",
    "        \n",
    "        question = complete_question(context, \"After\", full_answer)\n",
    "        answer = satellite.strip()\n",
    "\n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "\n",
    "nuc = \"and the world is enveloped in flames,\"\n",
    "sate = \"only to be reborn anew.\"\n",
    "sub_context = get_subtext(original_text, nuc, 500) # get 500 words\n",
    "\n",
    "# generate_temporal_question(sub_context, nuc, sate, use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elaboration_question(context, nucleus, satellite, use_template=False):\n",
    "    \"\"\"Generate a question based on the Elaboration relation.\n",
    "    Satellite (additional information) is the answer.\n",
    "    Nucleus (basic information) forms the questiom.\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        nucleus (str): \n",
    "        satellite (str): \n",
    "        use_template (bool, optional): Defaults to False.\n",
    "    \"\"\"\n",
    "    nucleus_pair = preprocessing_pipeline(context, nucleus, add_subject_to_rel_clause=True, \n",
    "                                          resolve_coref=True)\n",
    "    satellite_pair = preprocessing_pipeline(context, satellite, \n",
    "                                            add_subject_to_rel_clause=False, resolve_coref=False)\n",
    "    # to feed to complete question model for full context\n",
    "    full_nucleus_pair = preprocessing_pipeline(context, nucleus, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    full_satellite_pair = preprocessing_pipeline(context, satellite, retain_dm=True, \n",
    "                                          retain_conj=True, retain_adv=True, \n",
    "                                          add_subject_to_rel_clause=True, resolve_coref=True)\n",
    "    # if cannot process one of the two\n",
    "    if nucleus_pair[1] is None or satellite_pair[1] is None:\n",
    "        return (False, (\"\", \"\"))\n",
    "\n",
    "    # extract nucleus and satellite\n",
    "    nucleus = nucleus_pair[1]\n",
    "    satellite = satellite_pair[1]\n",
    "\n",
    "    # if use template\n",
    "    if use_template:\n",
    "       # WORKING: make template for this relation\n",
    "        return (False, (\"\", \"\"))\n",
    "    # not use template -> use neural question generator\n",
    "    else:   \n",
    "        # from observations, answer makes sense the most when is one sentence\n",
    "        if not is_single_sentence(satellite.strip()):\n",
    "            return (False, (\"\",\"\"))\n",
    "        # currently take full_answer (to feed to complete question model) to be the same as answer, \\\n",
    "        # which is basically the same as original nucleus, with no context added, \\\n",
    "        # might change later if needed.\n",
    "\n",
    "        # full_answer = full_nucleus_pair[1].strip()\n",
    "        full_answer = satellite.strip()\n",
    "        \n",
    "        key = \"What\"\n",
    "        last_noun_chunk = get_last_noun_chunk(full_nucleus_pair[1])\n",
    "        if last_noun_chunk is not None:\n",
    "            key = key + ' ' + last_noun_chunk\n",
    "\n",
    "        question = complete_question(context, key, full_answer) # don't change \"What\", need to be general like this\n",
    "        answer = satellite.strip()\n",
    "\n",
    "        question = postprocess_question(question)\n",
    "        answer = postprocess_answer(answer)\n",
    "        return (True, (question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "nuc = \"\"\"yet at the price of his future doom. Their father is the powerful god Njoror.\"\"\"\n",
    "sate = \"Njoror is strongly associated with ships and seafaring, and so also wealth and prosperity.\"\n",
    "sub_context = get_subtext(original_text, nuc, 500) # get 500 words\n",
    "\n",
    "# generate_elaboration_question(sub_context, nuc, sate, use_template=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test run for Background\n",
    "# rel = 'Background'\n",
    "# for trip in df[df['new_relation'] == rel].iterrows():\n",
    "#     with open(output_path, 'a') as f:\n",
    "#         f.write(\"\\nRELATION: \" + rel + '\\n')\n",
    "\n",
    "#         f.write(\"\\nOriginal nucleus: \" + trip[1]['nucleus'] + '\\n')\n",
    "#         f.write(\"Original satellite: \" + trip[1]['satellite'] + '\\n')\n",
    "#         f.write('\\n')\n",
    "        \n",
    "#         ques, ans = generate_background_question(original_text, trip[1]['nucleus'], trip[1]['satellite'])\n",
    "#         if not ques.strip() or not ans.strip():\n",
    "#             continue\n",
    "        \n",
    "#         f.write(\"\\nQuestion: \" + ques + '\\n')\n",
    "#         f.write(\"Answer: \" + ans + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DIS_SOURCE_LENGTH = 600\n",
    "MAX_DIS_TARGET_LENGTH = 256\n",
    "PREFIX = \"make 3 distractors:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from local dir\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "dis_model_path = \"../models/t5_base_distractors_generation_with_synthesized_dataset_custom_loss_sep_token_max_len_600/checkpoint-24000\"\n",
    "\n",
    "if 'dis_tokenizer' not in locals(): # prevent accidental re-run of cell\n",
    "    dis_tokenizer = AutoTokenizer.from_pretrained(dis_model_path)\n",
    "if 'dis_model' not in locals(): # prevent accidental re-run of cell\n",
    "    dis_model = T5ForConditionalGeneration.from_pretrained(dis_model_path)\n",
    "    dis_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def postprocess_distractor(dis):\n",
    "    \"\"\"Post process generated distractors.\n",
    "    Pipeline: Remove model's tags, remove redundant spaces.\n",
    "\n",
    "    Args:\n",
    "        dis (str): generated distractor\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned distractor\n",
    "    \"\"\"\n",
    "\n",
    "    new_dis = dis\n",
    "    special_tags = ['</s>', '<unk>', '<pad>']\n",
    "    for tag in special_tags:\n",
    "        new_dis = new_dis.replace(tag, '') \n",
    "\n",
    "    new_dis = re.sub(r'\\s+', ' ', new_dis)\n",
    "\n",
    "    new_dis_c = list(new_dis)\n",
    "    for i in range(len(new_dis_c)):\n",
    "        if len(new_dis_c[i].strip()) == 0:\n",
    "            continue\n",
    "        new_dis_c[i] = new_dis_c[i].upper()\n",
    "        new_dis_c = new_dis_c[i:]\n",
    "        break\n",
    "    return ''.join(new_dis_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def generate_3_distractors(context, question, answer):\n",
    "    \"\"\"Generate 3 distractors, needs globally available model and tokenizer (\"dis_model\" and \"dis_tokenizer\")\n",
    "\n",
    "    Args:\n",
    "        context (str): \n",
    "        question (str): \n",
    "        answer (str): \n",
    "    Return:\n",
    "        Tuple(dis1, dis2, dis3)\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = dis_tokenizer(text=f\"{PREFIX} question: {question}, answer: {answer}, context: {context}\", \n",
    "                        max_length=MAX_DIS_SOURCE_LENGTH,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt').to('cuda')\n",
    "    \n",
    "    output_sequences = dis_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=MAX_DIS_TARGET_LENGTH)[0]\n",
    "    \n",
    "    # find sep tokens, sep tokens separate among distractors\n",
    "    output_sequences = [token for token in output_sequences if token not in dis_tokenizer.convert_tokens_to_ids(['<pad>', '</s>'])]\n",
    "    sep_ids = [i for i, v in enumerate(output_sequences) if v == dis_tokenizer.convert_tokens_to_ids(['<sep>'])[0]]\n",
    "    try:\n",
    "        assert len(sep_ids) == 2\n",
    "    except:\n",
    "        print(\"Not enough seperation tokens were found!\")\n",
    "        return (\"\", \"\", \"\")\n",
    "    \n",
    "    # remove other special tokens\n",
    "    dis1 = dis_tokenizer.batch_decode([output_sequences[:sep_ids[0]]])[0]\n",
    "    dis2 = dis_tokenizer.batch_decode([output_sequences[sep_ids[0] + 1:sep_ids[1]]])[0]\n",
    "    dis3 = dis_tokenizer.batch_decode([output_sequences[sep_ids[1] + 1:]])[0]\n",
    "    \n",
    "    return (dis1, dis2, dis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "\n",
    "question = \"\"\"Why do machine learning algorithms perform tasks without explicit instructions?\"\"\"\n",
    "answer = \"\"\"That can learn from data and generalize to unseen data.\"\"\"\n",
    "\n",
    "# generate_3_distractors(original_text, question, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_UPPER_BOUND = 35\n",
    "LENGTH_LOWER_BOUND = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cause_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate cause question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Cause\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "       \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "\n",
    "        flag, (ques, ans) = generate_cause_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    \n",
    "    return questions_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate explanation question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Explanation\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_cause_question(sub_context, satellite, nucleus)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_contrast_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate contrast question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Contrast\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_contrast_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_condition_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate condition question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Condition\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_condition_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enablement_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate enablement question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Enablement\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_enablement_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_means_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate means question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Manner-Means\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_means_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate temporal question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Temporal\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_temporal_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elaboration_question_and_distractors(questions_df, discourse_df):\n",
    "    \"\"\"Generate elaboration question and correponding distractors, and append it to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        questions_df (DataFrame):\n",
    "    \"\"\"\n",
    "    rel = \"Elaboration\"\n",
    "    for trip in discourse_df[discourse_df['new_relation'] == rel].iterrows():\n",
    "        nucleus = trip[1]['nucleus'].strip()\n",
    "        satellite = trip[1]['satellite'].strip()\n",
    "        \n",
    "        if len(nucleus.split()) > LENGTH_UPPER_BOUND or len(satellite.split()) > LENGTH_UPPER_BOUND or len(nucleus.split()) < LENGTH_LOWER_BOUND or len(satellite.split()) < LENGTH_LOWER_BOUND:\n",
    "            continue\n",
    "\n",
    "        sub_context = get_subtext(original_text, nucleus, 500) # get 500 words\n",
    "        flag, (ques, ans) = generate_elaboration_question(sub_context, nucleus, satellite)\n",
    "        if not flag:\n",
    "            continue\n",
    "        \n",
    "        # generate distractors\n",
    "        dis1, dis2, dis3 = generate_3_distractors(sub_context, ques, ans)\n",
    "\n",
    "        datapoint = {'relation': [rel], 'nucleus': [nucleus], 'satellite': [satellite], 'question': [ques], 'answer': [ans], 'distractor1': [dis1], 'distractor2': [dis2], 'distractor3': [dis3]}\n",
    "        questions_df = pd.concat([questions_df, pd.DataFrame(datapoint)], ignore_index=True)\n",
    "    \n",
    "    if discourse_df[discourse_df['new_relation'] == rel].shape[0] == 0:\n",
    "        print(f\"No nucleus-satellite pair found for relation {rel}\\n\")\n",
    "    return questions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = split_into_sentences(original_text) # text split into sentences\n",
    "paras = split_into_paras(raw_original_text, deli='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.DataFrame(columns=['relation', 'nucleus', 'satellite', 'question', 'answer', 'distractor1', 'distractor2', 'distractor3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/04/2024 01:22:32 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nucleus-satellite pair found for relation Cause\n",
      "\n",
      "No nucleus-satellite pair found for relation Explanation\n",
      "\n",
      "No nucleus-satellite pair found for relation Contrast\n",
      "\n",
      "No nucleus-satellite pair found for relation Condition\n",
      "\n",
      "No nucleus-satellite pair found for relation Enablement\n",
      "\n",
      "No nucleus-satellite pair found for relation Manner-Means\n",
      "\n",
      "No nucleus-satellite pair found for relation Temporal\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 1/1 [00:00<00:00, 83.43 examples/s]\n",
      "07/04/2024 01:22:32 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  5.92it/s]\n",
      "07/04/2024 01:22:33 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 107.64 examples/s]\n",
      "07/04/2024 01:22:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.30it/s]\n",
      "07/04/2024 01:22:34 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 96.71 examples/s]\n",
      "07/04/2024 01:22:34 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.23it/s]\n",
      "07/04/2024 01:22:37 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 113.73 examples/s]\n",
      "07/04/2024 01:22:37 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.50it/s]\n",
      "07/04/2024 01:22:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 111.51 examples/s]\n",
      "07/04/2024 01:22:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  7.98it/s]\n",
      "07/04/2024 01:22:38 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 104.14 examples/s]\n",
      "07/04/2024 01:22:38 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.64it/s]\n",
      "07/04/2024 01:22:39 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 110.67 examples/s]\n",
      "07/04/2024 01:22:39 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.60it/s]\n",
      "07/04/2024 01:22:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 114.16 examples/s]\n",
      "07/04/2024 01:22:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.47it/s]\n",
      "07/04/2024 01:22:40 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 96.82 examples/s]\n",
      "07/04/2024 01:22:40 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.35it/s]\n",
      "07/04/2024 01:22:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 120.99 examples/s]\n",
      "07/04/2024 01:22:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Can't find subject for there is some economic crisis. Return the original clause.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/04/2024 01:22:42 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 129.98 examples/s]\n",
      "07/04/2024 01:22:42 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.72it/s]\n",
      "07/04/2024 01:22:43 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Can't find subject for where there is some economic crisis. Return the original clause.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 1/1 [00:00<00:00, 121.34 examples/s]\n",
      "07/04/2024 01:22:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.52it/s]\n",
      "07/04/2024 01:22:43 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 113.04 examples/s]\n",
      "07/04/2024 01:22:43 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.69it/s]\n",
      "07/04/2024 01:22:44 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 129.55 examples/s]\n",
      "07/04/2024 01:22:44 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.59it/s]\n",
      "07/04/2024 01:22:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 145.11 examples/s]\n",
      "07/04/2024 01:22:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.65it/s]\n",
      "07/04/2024 01:22:45 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 152.32 examples/s]\n",
      "07/04/2024 01:22:45 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  9.13it/s]\n",
      "07/04/2024 01:22:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 158.40 examples/s]\n",
      "07/04/2024 01:22:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.98it/s]\n",
      "07/04/2024 01:22:46 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|| 1/1 [00:00<00:00, 159.42 examples/s]\n",
      "07/04/2024 01:22:46 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference: 100%|| 1/1 [00:00<00:00,  8.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# full run of 8 relations: Cause, Explanation, Contrast, Condition, Enablement, \n",
    "# Manner-Means, Temporal, Elaboration\n",
    "\n",
    "questions = generate_cause_question_and_distractors(questions, df)\n",
    "questions = generate_explanation_question_and_distractors(questions, df)\n",
    "questions = generate_contrast_question_and_distractors(questions, df)\n",
    "questions = generate_condition_question_and_distractors(questions, df)\n",
    "questions = generate_enablement_question_and_distractors(questions, df)\n",
    "questions = generate_means_question_and_distractors(questions, df)\n",
    "questions = generate_temporal_question_and_distractors(questions, df)\n",
    "questions = generate_elaboration_question_and_distractors(questions, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = 'economic'\n",
    "questions_dest_path = \"generated_questions/my_model/{article_name}\"\n",
    "questions.to_csv(questions_dest_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_questions_file_path = './questions_economic_depression'\n",
    "# with open(full_questions_file_path, 'w') as f:\n",
    "#     f.write(f\"Context: {original_text}\\n\")\n",
    "#     f.write(\"--------------------\\n\")\n",
    "\n",
    "#     for row in questions.iterrows():\n",
    "#         # f.write(\"\\nNucleus: \" + nucleus + '\\n')\n",
    "#         # f.write(\"Satellite: \" + satellite + '\\n')\n",
    "\n",
    "#         f.write(\"\\nRelation: \" + row[1]['relation'] + '\\n')\n",
    "#         f.write(f\"\\nQuestion: {row[1]['question']}\")\n",
    "#         f.write(f\"\\nAnswer: {row[1]['answer']}\")\n",
    "#         f.write(f\"\\nDistractor 1: {row[1]['distractor1']}\")\n",
    "#         f.write(f\"\\nDistractor 2: {row[1]['distractor2']}\")\n",
    "#         f.write(f\"\\nDistractor 3: {row[1]['distractor3']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>nucleus</th>\n",
       "      <th>satellite</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>distractor1</th>\n",
       "      <th>distractor2</th>\n",
       "      <th>distractor3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elaboration</td>\n",
       "      <td>It is a result of more severe economic problems or a downturn than the recession itself,</td>\n",
       "      <td>which is a slowdown in economic activity over the course of the normal business cycle of growing economy.</td>\n",
       "      <td>What is an economic depression?</td>\n",
       "      <td>Is a slowdown in economic activity over the course of the normal business cycle of growing economy.</td>\n",
       "      <td>Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.</td>\n",
       "      <td>Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.</td>\n",
       "      <td>Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elaboration</td>\n",
       "      <td>that may be named economic depression are part of economic cycles</td>\n",
       "      <td>where the slowdown of the economy follows the economic growth and vice versa.</td>\n",
       "      <td>What are the economic cycles?</td>\n",
       "      <td>The slowdown of the economy follows the economic growth and vice versa.</td>\n",
       "      <td>The economy slows down and grows faster.</td>\n",
       "      <td>The economy slows down and grows faster.</td>\n",
       "      <td>The economy slows down and grows faster.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relation  \\\n",
       "0  Elaboration   \n",
       "1  Elaboration   \n",
       "\n",
       "                                                                                    nucleus  \\\n",
       "0  It is a result of more severe economic problems or a downturn than the recession itself,   \n",
       "1                         that may be named economic depression are part of economic cycles   \n",
       "\n",
       "                                                                                                   satellite  \\\n",
       "0  which is a slowdown in economic activity over the course of the normal business cycle of growing economy.   \n",
       "1                              where the slowdown of the economy follows the economic growth and vice versa.   \n",
       "\n",
       "                          question  \\\n",
       "0  What is an economic depression?   \n",
       "1    What are the economic cycles?   \n",
       "\n",
       "                                                                                                answer  \\\n",
       "0  Is a slowdown in economic activity over the course of the normal business cycle of growing economy.   \n",
       "1                              The slowdown of the economy follows the economic growth and vice versa.   \n",
       "\n",
       "                                                                                                                                     distractor1  \\\n",
       "0  Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.   \n",
       "1                                                                                                       The economy slows down and grows faster.   \n",
       "\n",
       "                                                                                                                                     distractor2  \\\n",
       "0  Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.   \n",
       "1                                                                                                       The economy slows down and grows faster.   \n",
       "\n",
       "                                                                                                                                     distractor3  \n",
       "0  Is a period of carried long - term economic downturn that is the result of lowered economic activity in one major or more national economies.  \n",
       "1                                                                                                       The economy slows down and grows faster.  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Generation for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_ques_len(data):\n",
    "    lens = []\n",
    "    for datapoint in data:\n",
    "        lens.append(len(datapoint.split()))\n",
    "    \n",
    "    return np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_dis_len(distractors):\n",
    "    lens = []\n",
    "    for datapoint in distractors:\n",
    "        lens.append(len(datapoint[0].split()))\n",
    "        lens.append(len(datapoint[1].split()))\n",
    "        lens.append(len(datapoint[2].split()))\n",
    "    \n",
    "    return np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "topic = \"personality\"\n",
    "model = 'MixQG'\n",
    "\n",
    "question_path = f'generated_questions/{model}/{topic}/questions_and_distractors_{topic}.csv'\n",
    "\n",
    "questions_df = pd.read_csv(question_path)\n",
    "\n",
    "# extract questions and answers\n",
    "questions = questions_df['question']\n",
    "answers = questions_df['answer']\n",
    "distractors = list(zip(questions_df['dis1'], questions_df['dis2'], questions_df['dis3'])) # if MixQG, the column is 'dis1' instead of 'distractor1' and vise versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 8.19718309859155, 3.0704225352112675, 3.76056338028169)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats\n",
    "len(questions), get_avg_ques_len(questions), get_avg_ques_len(answers), get_avg_dis_len(distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_question_prompt_for_article(article, questions, answers, task_description, prompt_question, include_context=True):\n",
    "    \"\"\"Make evaluation prompt for question generated on the article given. \n",
    "\n",
    "    Args:\n",
    "        article (str): \n",
    "        questions (list[str]): \n",
    "        answers (list[str]): \n",
    "    Returns:\n",
    "        str: the prompt to input to the evaluator LLM\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\"\"\"\n",
    "\n",
    "    for i, (question, answer) in enumerate(list(zip(questions, answers))):\n",
    "        prompt += (f\"Question {i + 1}: {question}, Answer {i + 1}: {answer}\\n\")\n",
    "    \n",
    "    if not include_context:\n",
    "        prompt = f\"\"\"{task_description}\\n\\n{prompt}\\n{prompt_question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"{task_description}\\n\\nContext: {article}\\n\\n{prompt}\\n{prompt_question}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distractor_prompt_for_article(article, questions, answers, distractors, task_description, prompt_question, include_context=True):\n",
    "    \"\"\"Make evaluation prompt for question generated on the article given. \n",
    "\n",
    "    Args:\n",
    "        article (str): \n",
    "        questions (list[str]): \n",
    "        answers (list[str]): \n",
    "        distractors (list[str]): \n",
    "    Returns:\n",
    "        str: the prompt to input to the evaluator LLM\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\"\"\"\n",
    "\n",
    "    for i, (question, answer) in enumerate(list(zip(questions, answers))):\n",
    "        prompt += (f\"Question {i + 1}: {question}, Answer {i + 1}: {answer}, Distractor 1: {distractors[i][0]}, Distractor 2: {distractors[i][1]}, Distractor 3: {distractors[i][2]}\\n\")\n",
    "    \n",
    "    if not include_context:\n",
    "        prompt = f\"\"\"{task_description}\\n\\n{prompt}\\n{prompt_question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"{task_description}\\n\\nContext: {article}\\n\\n{prompt}\\n{prompt_question}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for relevance \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/relevance_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the questions below. Below are a piece of context, answers, and corresponding questions based on the information in that context.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"How relevant are these questions to the context? (on a scale of 1 to 5, with 1 being completely irrelevant to the context). Output your results in the following manner: Question i: s/5 with i being the question number and s being the score.\"\"\"\n",
    "\n",
    "prompt = make_question_prompt_for_article(context, questions, answers, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for clarity \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/clarity_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the questions below. Below are a piece of context, answers, and corresponding questions based on the information in that context.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"A question is understandable when it uses clear and concise language and avoids complex or confusing terms. It should directly address the topic and be structured in a straightforward manner, ensuring that the test-taker can quickly grasp what is being asked without misinterpretation. How understandable are these questions? (on a scale of 1 to 5, where 1 being comfusing and impossible to be understood). Output your results in the following manner: Question i: s/5 with i being the question number and s being the score.\"\"\"\n",
    "\n",
    "prompt = make_question_prompt_for_article(context, questions, answers, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for answerability \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/answerability_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the questions below. Below are a piece of context, answers, and corresponding questions based on the information in that context.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"How answerable are these quesions given information in the context? (on a scale of 1 to 5, where 1 being completely unanswerable given the context). Output your results in the following manner: Question i: s/5 with i being the question number and s being the score.\"\"\"\n",
    "\n",
    "prompt = make_question_prompt_for_article(context, questions, answers, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for difficulty \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_prompt/MixQG/difficulty_prompt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtopic\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m task_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mA difficult question should require a higher level of cognitive ability and reasoning, they should not rely on too trivial factual details. On a scale of 1 to 5, evaluate these questions. Output your results in the following manner: Question i: s/5 with i being the question number and s being the score.\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mBelow are few example questions for each level: \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mEasy question: What is the capital city of France? (simple recall of factual information)\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mMedium question: How can the sun rise and set? (understanding the information rather than just recall)\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mDifficult question: How can urbanization lead to environmental degradation in developing countries? (require breaking down information, examining parts, and understanding relationships among them) Your task is to rate the questions below. Below are questions and answers based on the information in a piece of context.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m prompt_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic' is not defined"
     ]
    }
   ],
   "source": [
    "# for difficulty \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/difficulty_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the questions below. Below are a piece of context, answers, and corresponding questions based on the information in that context.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"A difficult question should require a higher level of cognitive ability and reasoning, they should not rely on too trivial factual details. How difficult are the questions? (on a scale of 1 to 5, where 1 are extremely simple questions that requires very low-level cognitive abilities). Output your results in the following manner: Question i: s/5 with i being the question number and s being the score.\n",
    "\n",
    "Below are few example questions for each level: \n",
    "Easy question: What is the capital city of France? (simple recall of factual information)\n",
    "Medium question: How can the sun rise and set? (understanding the information rather than just recall)\n",
    "Difficult question: How can urbanization lead to environmental degradation in developing countries? (require breaking down information, examining parts, and understanding relationships among them)\n",
    "\"\"\"\n",
    "\n",
    "prompt = make_question_prompt_for_article(context, questions, answers, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractors Generation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for relevance \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/distractor_incorrectness_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the distractors below. Below are a piece of context, questions, answers, and distractors for each question.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"Evaluate the distractors for their incorrectness. A score of 1 indicates that the distractor is correct or too close to the correct answer, making it ineffective as a distractor. A score of 5 indicates that the distractor is clearly incorrect and does not provide any ambiguity regarding the correct answer. Output your results in the following manner for each question: Question i: Distractor 1: s_1/5, Distractor 2: s_2/5, Distractor 3: s_3/5 with i being the question number and s_1, s_2, s_3 being the scores for each distractor.\"\"\"\n",
    "\n",
    "prompt = make_distractor_prompt_for_article(context, questions, answers, distractors, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for clarity \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/distractor_plausibility_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the distractors below. Below are a piece of context, questions, answers, and distractors for each question.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"Evaluate the plausibility of the distractors. A score of 1 suggests that the distractor is implausible or nonsensical, making it obvious that it is incorrect. A score of 5 indicates that the distractor is highly plausible and could be reasonably considered as the correct answer by someone who is unsure. Output your results in the following manner for each question: Question i: Distractor 1: s_1/5, Distractor 2: s_2/5, Distractor 3: s_3/5 with i being the question number and s_1, s_2, s_3 being the scores for each distractor.\"\"\"\n",
    "\n",
    "prompt = make_distractor_prompt_for_article(context, questions, answers, distractors, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for answerability \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/distractor_readability_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the distractors below. Below are a piece of context, questions, answers, and distractors for each question.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"Evaluate the readability of the distractors. A score of 1 means that the distractor is poorly written, confusing, or difficult to understand. A score of 5 indicates that the distractor is clear, concise, and easy to comprehend. Output your results in the following manner for each question: Question i: Distractor 1: s_1/5, Distractor 2: s_2/5, Distractor 3: s_3/5 with i being the question number and s_1, s_2, s_3 being the scores for each distractor.\"\"\"\n",
    "\n",
    "prompt = make_distractor_prompt_for_article(context, questions, answers, distractors, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for difficulty \n",
    "output_file_path = f\"evaluation_prompt/{model}/{topic}/distractor_diversity_prompt_{topic}.txt\"\n",
    "\n",
    "task_description = \"\"\"The task is to rate the distractors below. Below are a piece of context, questions, answers, and distractors for each question.\"\"\"\n",
    "\n",
    "prompt_question = \"\"\"Evaluate the diversity of the distractors. A score of 1 signifies that the distractors are too similar to each other or to the correct answer, providing little variety. A score of 5 means that the distractors are diverse, covering a range of different concepts or angles, making the question more challenging. Output your results in the following manner for each question: Question i: Distractor 1: s_1/5, Distractor 2: s_2/5, Distractor 3: s_3/5 with i being the question number and s_1, s_2, s_3 being the scores for each distractor.\"\"\"\n",
    "\n",
    "prompt = make_distractor_prompt_for_article(context, questions, answers, distractors, task_description, prompt_question)\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
